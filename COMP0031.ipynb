{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/interritus141/COMP0031-Group-Research-Project/blob/master/COMP0031.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ! pip install pettingzoo\n",
        "# ! pip install gymnasium\n",
        "# ! pip install stable_baselines3\n",
        "# ! pip install ta\n",
        "# ! pip install yfinance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ruZOtW7WfjEi"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "XbrhDC6YfjuP"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "from stable_baselines3 import A2C, DDPG, DQN, PPO\n",
        "from ta import add_all_ta_features\n",
        "import yfinance as yf\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "import functools\n",
        "import os\n",
        "import csv\n",
        "import json\n",
        "from math import prod"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Helpers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "x7RFmf4lj3Yt"
      },
      "outputs": [],
      "source": [
        "def plot_line_graph(env, x_vals, y_dict, title):\n",
        "  for agent in env.possible_agents:\n",
        "    plt.plot(x_vals, y_dict[agent], label=agent)\n",
        "  plt.title(title)\n",
        "  plt.legend()\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_sharpe(portfolio_return_mem):\n",
        "  df_daily_return = pd.DataFrame(portfolio_return_mem)\n",
        "  df_daily_return.columns = ['daily_return']\n",
        "  sharpe = -1 \n",
        "  if df_daily_return['daily_return'].std() != 0:\n",
        "    sharpe = (252**0.5)*df_daily_return['daily_return'].mean() / df_daily_return['daily_return'].std()\n",
        "  return sharpe # -1 means error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def save_to_json(filename, data_dict):\n",
        "  with open(filename, \"w\") as data_out:\n",
        "    json.dump(data_dict, data_out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "itPimO3WZcN_"
      },
      "source": [
        "# Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NdcDd-Iqd_-Z"
      },
      "source": [
        "## Tech Indicators"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "lbA-Y5Y3grjA"
      },
      "outputs": [],
      "source": [
        "def add_ta(df):\n",
        "  ta_df = add_all_ta_features(df, open=\"Open\", high=\"High\", low=\"Low\", close=\"Close\", volume=\"Volume\")\n",
        "  # print(ta_df.columns)\n",
        "  ta_df = ta_df[[\"Open\", \"High\", \"Low\", \"Close\", \"Volume\", \"Adj Close\", \"volume_obv\",\n",
        "                                \"volume_adi\", \"trend_adx\", \"momentum_ao\", \"trend_macd\", \"momentum_rsi\", \n",
        "                                \"momentum_stoch\"]]\n",
        "  ta_df = ta_df.fillna(ta_df.mean())\n",
        "  return ta_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ttJNGUWuvVF2"
      },
      "source": [
        "## Stocks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3JCv_b7iw9b"
      },
      "source": [
        "1. Apple Inc. (AAPL)\n",
        "2. Microsoft Corp. (MSFT)\n",
        "3. Amazon.com, Inc. ( AMZN)\n",
        "4. Tesla, Inc. (TSLA)\n",
        "5. Nvidia Corp. (NVDA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KQW7POdeiKTJ",
        "outputId": "ab402ff2-b8a2-4834-a616-e8484aba3b1c"
      },
      "outputs": [],
      "source": [
        "# interval = 1m,2m,5m,15m,30m,60m,90m,1h,1d,5d,1wk,1mo,3mo\n",
        "# prepost = T/F\n",
        "\n",
        "stock_volatilities = {}\n",
        "stocks = dict.fromkeys([\"AAPL\", \"MSFT\", \"AMZN\", \"TSLA\", \"NVDA\", \"CAAS\"])\n",
        "\n",
        "for stock in stocks.keys():\n",
        "  stock_df = yf.download(stock, start=\"2018-01-01\", end=\"2022-12-31\", keepna=True)\n",
        "  stock_df = stock_df.fillna(stock_df.mean())\n",
        "  stock_df = add_ta(stock_df)\n",
        "  stocks[stock] = stock_df\n",
        "  stock_volatilities[stock] = None"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Volatility"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def add_volatility(df, df_name):\n",
        "  df[\"Log returns\"] = np.log(df['Close'] / df['Close'].shift())\n",
        "  stock_volatilities[df_name] = df['Log returns'].std() * 252 ** .5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'AAPL': 0.33515159660010024, 'MSFT': 0.3108167320339789, 'AMZN': 0.35823866187984743, 'TSLA': 0.6545095345365393, 'NVDA': 0.5216282953491833, 'CAAS': 0.83977708799141}\n"
          ]
        }
      ],
      "source": [
        "def visualise_volatility(df, df_name, volatility):\n",
        "  fig, ax = plt.subplots()\n",
        "  df['Log returns'].hist(ax=ax, bins=50, alpha=0.6, color='b')\n",
        "  ax.set_xlabel(\"Log return\")\n",
        "  ax.set_ylabel(\"Freq of log return\")\n",
        "  ax.set_title(\"{:s} volatility: {:.2f}%\".format(df_name, volatility*100))\n",
        "\n",
        "for stock, stock_df in stocks.items():\n",
        "  add_volatility(stock_df, stock)\n",
        "  # visualise_volatility(stock_df, stock, stock_volatilities[stock]) # use to generate graphs\n",
        "\n",
        "print(stock_volatilities)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cov"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "uJtS7XLJuSs2"
      },
      "outputs": [],
      "source": [
        "def add_cov(df):\n",
        "  df = df.reset_index()\n",
        "\n",
        "  cov_list = []\n",
        "  return_list = []\n",
        "\n",
        "  # look back is one year\n",
        "  lookback=252\n",
        "  for i in range(lookback,len(df.index.unique())):\n",
        "    data_lookback = df.iloc[i-lookback:i,:]\n",
        "    price_lookback=data_lookback.pivot_table(index = 'Date', values = 'Close')\n",
        "    return_lookback = price_lookback.pct_change().dropna()\n",
        "    return_list.append(return_lookback)\n",
        "\n",
        "    covs = return_lookback.cov().values \n",
        "    cov_list.append(covs)\n",
        "\n",
        "\n",
        "  df_cov = pd.DataFrame({'Date':df[\"Date\"].unique()[lookback:],'cov_list':cov_list,'return_list':return_list})\n",
        "  df = df.merge(df_cov, on='Date')\n",
        "  df = df.sort_values(['Date']).reset_index(drop=True)\n",
        "  return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "QwlliL4VulGv"
      },
      "outputs": [],
      "source": [
        "# high volatility\n",
        "new_aapl_df = add_cov(stocks[\"AAPL\"])\n",
        "\n",
        "# low volatility\n",
        "new_tsla_df = add_cov(stocks[\"TSLA\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "8OADGaYLI-bv"
      },
      "outputs": [],
      "source": [
        "data_aapl_df = new_aapl_df.copy()\n",
        "data_aapl_df[\"tic\"] = \"AAPL\"\n",
        "\n",
        "data_tsla_df = new_tsla_df.copy()\n",
        "data_tsla_df[\"tic\"] = \"TSLA\"\n",
        "\n",
        "mixed_df = pd.concat([data_aapl_df, data_tsla_df])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mVYjY6QFj2Uy"
      },
      "source": [
        "# Environment"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "XIA9Tg7bmZGN"
      },
      "outputs": [],
      "source": [
        "policy = \"MlpPolicy\"\n",
        "training_timesteps = 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "AGENT_STR_TO_OBJECT = {\n",
        "    \"A2C\": A2C,\n",
        "    \"DDPG\": DDPG,\n",
        "    \"PPO\": PPO,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "TYPE_TO_NAMES = {\n",
        "    \"A2C\": [\"A2C1\", \"A2C2\", \"A2C3\"],\n",
        "    \"DDPG\": [\"DDPG1\", \"DDPG2\", \"DDPG3\"],\n",
        "    \"PPO\": [\"PPO1\", \"PPO2\", \"PPO3\"],\n",
        "    \"Mixed\": [\"A2C\", \"DDPG\", \"PPO\"],\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# configurations\n",
        "\n",
        "stock_dimension = len(mixed_df[\"tic\"].unique())\n",
        "state_space = 2\n",
        "\n",
        "env_kwargs = {\n",
        "  \"hmax\": 100, \n",
        "  \"initial_amount\": 1000000, \n",
        "  \"transaction_cost_pct\": 0.001, \n",
        "  \"state_space\": state_space, \n",
        "  \"stock_dim\": stock_dimension, \n",
        "  \"tech_indicator_list\": [\n",
        "    \"volume_obv\",\n",
        "    \"volume_adi\", \n",
        "    \"trend_adx\", \n",
        "    \"momentum_ao\", \n",
        "    \"trend_macd\", \n",
        "    \"momentum_rsi\", \n",
        "    \"momentum_stoch\"\n",
        "  ], \n",
        "  \"action_space\": stock_dimension, \n",
        "  \"reward_scaling\": 1e-4,\n",
        "    \n",
        "}"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Competitive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CompetitivePMEnv(gym.Env):\n",
        "  metadata = {\"render_modes\": [\"human\"], \"name\": \"marlpm_v1\"}\n",
        "\n",
        "  def __init__(\n",
        "      self, \n",
        "      df,\n",
        "      stock_dim,\n",
        "      hmax,\n",
        "      initial_amount,\n",
        "      transaction_cost_pct,\n",
        "      reward_scaling,\n",
        "      state_space,\n",
        "      action_space,\n",
        "      tech_indicator_list,\n",
        "      turbulence_threshold=None,\n",
        "      lookback=252,\n",
        "      day=0,\n",
        "      render_mode=None,\n",
        "      algo_type=\"Mixed\", # default=mixed\n",
        "  ):\n",
        "\n",
        "    assert algo_type in [\"A2C\", \"DDPG\", \"PPO\", \"Mixed\"]\n",
        "\n",
        "    # attributes\n",
        "    self.lookback=lookback\n",
        "    self.df = df\n",
        "    self.stock_dim = stock_dim\n",
        "    self.hmax = hmax\n",
        "    self.initial_amount = initial_amount\n",
        "    self.transaction_cost_pct =transaction_cost_pct\n",
        "    self.reward_scaling = reward_scaling\n",
        "    self.state_space = state_space\n",
        "    self.action_dim = action_space\n",
        "    self.tech_indicator_list = tech_indicator_list\n",
        "    self.possible_agents = TYPE_TO_NAMES[algo_type]\n",
        "    \n",
        "    # buy/sell ratio reference, to explore\n",
        "    self.end_day = len(self.df.index.unique()) - 1\n",
        "    self.stock_volume_reference = 10000\n",
        "\n",
        "    # spaces\n",
        "    # check: spaces for observations only? sharing will affect?\n",
        "    self.action_space = gym.spaces.Box(low = -1, high = 1, shape = (self.action_dim,))\n",
        "    self.observation_space = gym.spaces.Box(low=0, high=np.inf, shape = (1+len(self.tech_indicator_list), self.state_space))\n",
        "\n",
        "    # agents\n",
        "    self.agent_name_mapping = {\n",
        "        agent: AGENT_STR_TO_OBJECT[algo_type](policy, self, self.end_day+1) for agent in self.possible_agents\n",
        "    }\n",
        "    self.training_agent = None\n",
        "    self.day = {\n",
        "        agent: day for agent in self.possible_agents\n",
        "    }\n",
        "\n",
        "    # data\n",
        "    self.data = {\n",
        "        agent: self.df.loc[self.day[agent],:] for agent in self.possible_agents\n",
        "    }\n",
        "    self.covs = {\n",
        "        agent: [[x[0][0] for x in self.data[agent]['cov_list']]] for agent in self.possible_agents\n",
        "    }\n",
        "    self.state = {\n",
        "        agent: np.append(np.array(self.covs[agent]), [self.data[agent][tech].values.tolist() for tech in self.tech_indicator_list ], axis=0) for agent in self.possible_agents\n",
        "    }\n",
        "    self.terminal = False     \n",
        "    self.turbulence_threshold = turbulence_threshold   \n",
        "\n",
        "    # memory\n",
        "    self.portfolio_value = {\n",
        "        agent: self.initial_amount for agent in self.possible_agents\n",
        "    }\n",
        "    self.asset_memory = {\n",
        "        agent: [self.initial_amount] for agent in self.possible_agents\n",
        "    }\n",
        "    self.portfolio_return_memory = {\n",
        "        agent: [0] for agent in self.possible_agents\n",
        "    }\n",
        "    self.cum_portfolio_return_memory = {\n",
        "        agent: [0] for agent in self.possible_agents\n",
        "    }\n",
        "\n",
        "    # stock ratio\n",
        "    self.actions_memory = {\n",
        "        agent: [[0]*self.stock_dim] for agent in self.possible_agents\n",
        "    }\n",
        "    self.date_memory = {\n",
        "        agent: [self.data[agent][\"Date\"].unique()[0]] for agent in self.possible_agents\n",
        "    }\n",
        "\n",
        "    # free cash\n",
        "    self.money_memory = {\n",
        "        agent: [self.initial_amount] for agent in self.possible_agents\n",
        "    }\n",
        "    # cash + stock value\n",
        "    self.total_value_memory = {\n",
        "        agent: [self.initial_amount] for agent in self.possible_agents\n",
        "    }\n",
        "    # individual actions collection\n",
        "    self.individual_preds = {\n",
        "        agent: [] for agent in self.possible_agents\n",
        "    }\n",
        "\n",
        "    # render mode\n",
        "    self.render_mode = render_mode\n",
        "\n",
        "  def collect_individual_preds(self):\n",
        "    self.individual_preds = {\n",
        "        agent: [] for agent in self.possible_agents\n",
        "    }\n",
        "    for i in range(self.end_day+1):\n",
        "      # states are somewhat static\n",
        "      curr_data = self.df.loc[i,:]\n",
        "      curr_covs = [[x[0][0] for x in curr_data['cov_list']]]\n",
        "      curr_state = np.append(np.array(curr_covs), [curr_data[tech].values.tolist() for tech in self.tech_indicator_list ], axis=0)\n",
        "      for agent in self.possible_agents:\n",
        "        action, _states = self.agent_name_mapping[agent].predict(curr_state)\n",
        "        self.individual_preds[agent].append(action)\n",
        "  \n",
        "  def step(self, actions):\n",
        "\n",
        "    # termination\n",
        "    self.terminal = self.day[self.training_agent] >= self.end_day\n",
        "\n",
        "    if self.terminal:\n",
        "\n",
        "      # uncomment to print metrics on terminal\n",
        "      # print(\"=================================\")\n",
        "      # print(\"begin_money:{}\".format(self.asset_memory[self.training_agent][0]))           \n",
        "      # print(\"end_daily_total_portfolio_value:{}\".format(self.total_value_memory[self.training_agent][-1]))\n",
        "      # print(\"end_daily_portfolio_return:{}\".format(self.portfolio_return_memory[self.training_agent][-1]))\n",
        "      # print(\"end_cumulative_portfolio_return:{}\".format(self.cum_portfolio_return_memory[self.training_agent][-1]))\n",
        "\n",
        "      # df_daily_return = pd.DataFrame(self.portfolio_return_memory[self.training_agent])\n",
        "      # df_daily_return.columns = ['daily_return']\n",
        "      # if df_daily_return['daily_return'].std() !=0:\n",
        "      #   sharpe = (252**0.5)*df_daily_return['daily_return'].mean() / df_daily_return['daily_return'].std()\n",
        "      #   print(\"Sharpe: \",sharpe)\n",
        "      # print(\"=================================\")\n",
        "\n",
        "      return self.state[self.training_agent], self.reward[self.training_agent], self.terminal, {}\n",
        "\n",
        "    else:\n",
        "      # loop through all agents so that each of them predict an action (portfolio weights)\n",
        "      for agent in self.possible_agents:\n",
        "        # get action\n",
        "        if agent == self.training_agent:\n",
        "          action = actions\n",
        "        else:\n",
        "          # action, _states = self.agent_name_mapping[agent].predict(self.state[agent], deterministic=False)\n",
        "          action = self.individual_preds[agent][self.day[agent]]\n",
        "\n",
        "        # normalisation\n",
        "        weights = self.softmax_normalization(action) \n",
        "\n",
        "        # stock ratio - buy/sell/hold\n",
        "        prev_stock_ratio = self.actions_memory[agent][-1]\n",
        "        diff_stock_ratio = prev_stock_ratio - weights\n",
        "\n",
        "        # money - increase if sell, decrease if buy, no changes if hold\n",
        "        prev_money = self.money_memory[agent][-1]\n",
        "        curr_money = prev_money + sum(diff_stock_ratio * self.stock_volume_reference * self.data[agent][\"Close\"].values)\n",
        "        self.money_memory[agent].append(curr_money)\n",
        "\n",
        "        # total value - money + currently held stock value\n",
        "        curr_total = curr_money + sum(weights * self.stock_volume_reference * self.data[agent][\"Close\"].values)\n",
        "        self.total_value_memory[agent].append(curr_total)\n",
        "\n",
        "        # actions memory\n",
        "        self.actions_memory[agent].append(weights)\n",
        "        last_day_memory = self.data[agent]\n",
        "\n",
        "        # load next state\n",
        "        self.day[agent] += 1\n",
        "        self.data[agent] = self.df.loc[self.day[agent],:]\n",
        "        self.covs[agent] = [[x[0][0] for x in self.data[agent]['cov_list']]]\n",
        "        self.state[agent] =  np.append(np.array(self.covs[agent]), [self.data[agent][tech].values.tolist() for tech in self.tech_indicator_list ], axis=0)\n",
        "        \n",
        "        # calculate portfolio return\n",
        "        # individual stocks' return * weight\n",
        "        portfolio_return = sum(((self.data[agent][\"Close\"].values / last_day_memory[\"Close\"].values)-1)*weights)\n",
        "        \n",
        "        # save into memory\n",
        "        self.portfolio_return_memory[agent].append(portfolio_return)\n",
        "        self.cum_portfolio_return_memory[agent].append(self.cum_portfolio_return_memory[agent][-1] + portfolio_return)\n",
        "        self.date_memory[agent].append(self.data[agent][\"Date\"].unique()[0])            \n",
        "        self.asset_memory[agent].append(curr_total)\n",
        "\n",
        "        # the reward is the new portfolio value or end portfolio value\n",
        "        self.reward[agent] = curr_total \n",
        "        #self.reward = self.reward*self.reward_scaling\n",
        "      \n",
        "      # penalise or reward the target agent based on the result of all other agents\n",
        "      all_rewards = list(self.reward.values())\n",
        "      for agent in self.possible_agents:\n",
        "        # ratio = current agent / other agent\n",
        "        # if reward of current agent > other agent, ratio > 1, reward is increased\n",
        "        # else ratio < 1, reward is penalised\n",
        "        self.reward[agent] *= prod(self.reward[agent] / all_rewards)\n",
        "\n",
        "        # if money on hand is negative, large penalty is applied as this is unwanted\n",
        "        if self.money_memory[agent][-1] < 0:\n",
        "          self.reward[agent] *= -1\n",
        "\n",
        "    return self.state[agent], self.reward[self.training_agent], self.terminal, {}\n",
        "\n",
        "  def reset(self, seed=None, return_info=False, options=None):\n",
        "    # print(\"reset\")\n",
        "\n",
        "    # agents\n",
        "    self.agents = self.possible_agents[:]\n",
        "\n",
        "    # attributes\n",
        "    self.day = {\n",
        "        agent: 0 for agent in self.possible_agents\n",
        "    }\n",
        "    self.data = {\n",
        "        agent: self.df.loc[self.day[agent],:] for agent in self.possible_agents\n",
        "    }\n",
        "    self.covs = {\n",
        "        agent: [[x[0][0] for x in self.data[agent]['cov_list']]] for agent in self.possible_agents\n",
        "    }\n",
        "    self.state = {\n",
        "        agent: np.append(np.array(self.covs[agent]), [self.data[agent][tech].values.tolist() for tech in self.tech_indicator_list ], axis=0) for agent in self.possible_agents\n",
        "    }\n",
        "\n",
        "    # memory\n",
        "    self.portfolio_value = {\n",
        "        agent: self.initial_amount for agent in self.possible_agents\n",
        "    }\n",
        "    self.asset_memory = {\n",
        "        agent: [self.initial_amount] for agent in self.possible_agents\n",
        "    }\n",
        "    self.portfolio_return_memory = {\n",
        "        agent: [0] for agent in self.possible_agents\n",
        "    }\n",
        "    self.cum_portfolio_return_memory = {\n",
        "        agent: [0] for agent in self.possible_agents\n",
        "    }\n",
        "\n",
        "    # stock ratio\n",
        "    self.actions_memory = {\n",
        "        agent: [[0]*self.stock_dim] for agent in self.possible_agents\n",
        "    }\n",
        "    self.date_memory = {\n",
        "        agent: [self.data[agent][\"Date\"].unique()[0]] for agent in self.possible_agents\n",
        "    }\n",
        "\n",
        "    # free cash\n",
        "    self.money_memory = {\n",
        "        agent: [self.initial_amount] for agent in self.possible_agents\n",
        "    }\n",
        "    # cash + stock value\n",
        "    self.total_value_memory = {\n",
        "        agent: [self.initial_amount] for agent in self.possible_agents\n",
        "    }\n",
        "\n",
        "    # rewards\n",
        "    self.reward = {\n",
        "        agent: None for agent in self.possible_agents\n",
        "    }\n",
        "    \n",
        "    # misc\n",
        "    self.terminal = False \n",
        "    #self.cost = 0\n",
        "    #self.trades = 0\n",
        "    \n",
        "    return self.state[self.training_agent] \n",
        "\n",
        "  def render(self):\n",
        "    return self.state[self.training_agent]\n",
        "\n",
        "  def seed(self, seed=None):\n",
        "    self.np_random, seed = seeding.np_random(seed)\n",
        "    return [seed]\n",
        "  \n",
        "  def softmax_normalization(self, actions):\n",
        "    numerator = np.exp(actions)\n",
        "    denominator = np.sum(np.exp(actions))\n",
        "    softmax_output = numerator/denominator\n",
        "    return softmax_output\n",
        "\n",
        "  def set_training_agent(self, agent):\n",
        "    # print(agent)\n",
        "    self.training_agent = agent\n",
        "\n",
        "  def learn(self, total_timesteps=1000):\n",
        "    init_pv = {}\n",
        "    final_pv = {}\n",
        "    init_cash = {}\n",
        "    final_cash = {}\n",
        "    init_daily_pr = {}\n",
        "    final_daily_pr = {}\n",
        "    init_cum_pr = {}\n",
        "    final_cum_pr = {}\n",
        "    sharpe_ratio = {\n",
        "        agent: [] for agent in self.possible_agents\n",
        "    }\n",
        "\n",
        "    # run till terminal in each timestep\n",
        "    for n in range(total_timesteps):\n",
        "      print(\"Step:\", n+1)\n",
        "      self.collect_individual_preds()\n",
        "      for agent in self.possible_agents:\n",
        "        self.set_training_agent(agent)\n",
        "        self.agent_name_mapping[agent] = self.agent_name_mapping[agent].learn(total_timesteps=1)\n",
        "        sharpe_ratio[agent].append(calculate_sharpe(self.portfolio_return_memory[agent]))\n",
        "        \n",
        "      if n == 0:\n",
        "        # save init for plot\n",
        "        for agent in self.possible_agents:\n",
        "          init_pv[agent] = self.total_value_memory[agent]\n",
        "          init_daily_pr[agent] = self.portfolio_return_memory[agent]\n",
        "          init_cum_pr[agent] = self.cum_portfolio_return_memory[agent]\n",
        "          init_cash[agent] = self.money_memory[agent]\n",
        "    \n",
        "    # save final for plot\n",
        "    for agent in self.possible_agents:\n",
        "      final_pv[agent] = self.total_value_memory[agent]\n",
        "      final_daily_pr[agent] = self.portfolio_return_memory[agent]\n",
        "      final_cum_pr[agent] = self.cum_portfolio_return_memory[agent]\n",
        "      final_cash[agent] = self.money_memory[agent]\n",
        "    \n",
        "    return init_pv, final_pv, init_cash, final_cash, init_daily_pr, final_daily_pr, init_cum_pr, final_cum_pr, sharpe_ratio\n",
        "\n",
        "  def save_weights(self):\n",
        "    for idx, agent in enumerate(self.possible_agents):\n",
        "      self.agent_name_mapping[agent].save(\"comp_{}_{}.zip\".format(agent, training_timesteps))\n",
        "\n",
        "  def load_weights(self):\n",
        "    for idx, agent in enumerate(self.possible_agents):\n",
        "      self.agent_name_mapping[agent].set_parameters(\"comp_{}_{}.zip\".format(agent, training_timesteps))\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### A2C"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "a2c_comp_env = CompetitivePMEnv(df=mixed_df, algo_type=\"A2C\", **env_kwargs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "(\n",
        "    a2c_comp_init_pv, \n",
        "    a2c_comp_final_pv, \n",
        "    a2c_comp_init_cash, \n",
        "    a2c_comp_final_cash, \n",
        "    a2c_comp_init_daily_pr, \n",
        "    a2c_comp_final_daily_pr, \n",
        "    a2c_comp_init_cum_pr, \n",
        "    a2c_comp_final_cum_pr, \n",
        "    a2c_comp_sr\n",
        ") = a2c_comp_env.learn(\n",
        "    total_timesteps=training_timesteps,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# save_to_json(\"a2c_comp_init_pv_{}.json\".format(training_timesteps), a2c_comp_init_pv)\n",
        "# save_to_json(\"a2c_comp_final_pv_{}.json\".format(training_timesteps), a2c_comp_final_pv)\n",
        "\n",
        "# save_to_json(\"a2c_comp_init_cash_{}.json\".format(training_timesteps), a2c_comp_init_cash)\n",
        "# save_to_json(\"a2c_comp_final_cash_{}.json\".format(training_timesteps), a2c_comp_final_cash)\n",
        "\n",
        "# save_to_json(\"a2c_comp_init_daily_pr_{}.json\".format(training_timesteps), a2c_comp_init_daily_pr)\n",
        "# save_to_json(\"a2c_comp_final_daily_pr_{}.json\".format(training_timesteps), a2c_comp_final_daily_pr)\n",
        "\n",
        "# save_to_json(\"a2c_comp_init_cum_pr_{}.json\".format(training_timesteps), a2c_comp_init_cum_pr)\n",
        "# save_to_json(\"a2c_comp_final_cum_pr_{}.json\".format(training_timesteps), a2c_comp_final_cum_pr)\n",
        "\n",
        "# save_to_json(\"a2c_comp_sr_{}.json\".format(training_timesteps), a2c_comp_sr)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Save weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "a2c_comp_env.save_weights()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load from pre-trained weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "a2c_load_comp = CompetitivePMEnv(df=mixed_df, algo_type=\"A2C\", **env_kwargs)\n",
        "a2c_load_comp.load_weights()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### DDPG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ddpg_comp_env = CompetitivePMEnv(df=mixed_df, algo_type=\"DDPG\", **env_kwargs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "(\n",
        "    ddpg_comp_init_pv, \n",
        "    ddpg_comp_final_pv, \n",
        "    ddpg_comp_init_cash, \n",
        "    ddpg_comp_final_cash, \n",
        "    ddpg_comp_init_daily_pr, \n",
        "    ddpg_comp_final_daily_pr, \n",
        "    ddpg_comp_init_cum_pr, \n",
        "    ddpg_comp_final_cum_pr, \n",
        "    ddpg_comp_sr\n",
        ") = ddpg_comp_env.learn(\n",
        "    total_timesteps=training_timesteps,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# save_to_json(\"ddpg_comp_init_pv_{}.json\".format(training_timesteps), ddpg_comp_init_pv)\n",
        "# save_to_json(\"ddpg_comp_final_pv_{}.json\".format(training_timesteps), ddpg_comp_final_pv)\n",
        "\n",
        "# save_to_json(\"ddpg_comp_init_cash_{}.json\".format(training_timesteps), ddpg_comp_init_cash)\n",
        "# save_to_json(\"ddpg_comp_final_cash_{}.json\".format(training_timesteps), ddpg_comp_final_cash)\n",
        "\n",
        "# save_to_json(\"ddpg_comp_init_daily_pr_{}.json\".format(training_timesteps), ddpg_comp_init_daily_pr)\n",
        "# save_to_json(\"ddpg_comp_final_daily_pr_{}.json\".format(training_timesteps), ddpg_comp_final_daily_pr)\n",
        "\n",
        "# save_to_json(\"ddpg_comp_init_cum_pr_{}.json\".format(training_timesteps), ddpg_comp_init_cum_pr)\n",
        "# save_to_json(\"ddpg_comp_final_cum_pr_{}.json\".format(training_timesteps), ddpg_comp_final_cum_pr)\n",
        "\n",
        "# save_to_json(\"ddpg_comp_sr_{}.json\".format(training_timesteps), ddpg_comp_sr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Save weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ddpg_comp_env.save_weights()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load from pre-trained weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ddpg_load_comp = CompetitivePMEnv(df=mixed_df, algo_type=\"DDPG\", **env_kwargs)\n",
        "ddpg_load_comp.load_weights()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### PPO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ppo_comp_env = CompetitivePMEnv(df=mixed_df, algo_type=\"PPO\", **env_kwargs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "(\n",
        "    ppo_comp_init_pv, \n",
        "    ppo_comp_final_pv, \n",
        "    ppo_comp_init_cash, \n",
        "    ppo_comp_final_cash, \n",
        "    ppo_comp_init_daily_pr, \n",
        "    ppo_comp_final_daily_pr, \n",
        "    ppo_comp_init_cum_pr, \n",
        "    ppo_comp_final_cum_pr, \n",
        "    ppo_comp_sr\n",
        ") = ppo_comp_env.learn(\n",
        "    total_timesteps=training_timesteps,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# save_to_json(\"ppo_comp_init_pv_{}.json\".format(training_timesteps), ppo_comp_init_pv)\n",
        "# save_to_json(\"ppo_comp_final_pv_{}.json\".format(training_timesteps), ppo_comp_final_pv)\n",
        "\n",
        "# save_to_json(\"ppo_comp_init_cash_{}.json\".format(training_timesteps), ppo_comp_init_cash)\n",
        "# save_to_json(\"ppo_comp_final_cash_{}.json\".format(training_timesteps), ppo_comp_final_cash)\n",
        "\n",
        "# save_to_json(\"ppo_comp_init_daily_pr_{}.json\".format(training_timesteps), ppo_comp_init_daily_pr)\n",
        "# save_to_json(\"ppo_comp_final_daily_pr_{}.json\".format(training_timesteps), ppo_comp_final_daily_pr)\n",
        "\n",
        "# save_to_json(\"ppo_comp_init_cum_pr_{}.json\".format(training_timesteps), ppo_comp_init_cum_pr)\n",
        "# save_to_json(\"ppo_comp_final_cum_pr_{}.json\".format(training_timesteps), ppo_comp_final_cum_pr)\n",
        "\n",
        "# save_to_json(\"ppo_comp_sr_{}.json\".format(training_timesteps), ppo_comp_sr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Save weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ppo_comp_env.save_weights()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load from pre-trained weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ppo_load_comp = CompetitivePMEnv(df=mixed_df, algo_type=\"PPO\", **env_kwargs)\n",
        "ppo_load_comp.load_weights()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cooperative"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CooperativePMEnv(gym.Env):\n",
        "  metadata = {\"render_modes\": [\"human\"], \"name\": \"marlpm_v1\"}\n",
        "\n",
        "  def __init__(\n",
        "      self, \n",
        "      df,\n",
        "      stock_dim,\n",
        "      hmax,\n",
        "      initial_amount,\n",
        "      transaction_cost_pct,\n",
        "      reward_scaling,\n",
        "      state_space,\n",
        "      action_space,\n",
        "      tech_indicator_list,\n",
        "      turbulence_threshold=None,\n",
        "      lookback=252,\n",
        "      day=0,\n",
        "      render_mode=None,\n",
        "      algo_type=\"Mixed\", # default=mixed\n",
        "  ):\n",
        "\n",
        "    assert algo_type in [\"A2C\", \"DDPG\", \"PPO\", \"Mixed\"]\n",
        "\n",
        "    # attributes\n",
        "    self.lookback=lookback\n",
        "    self.df = df\n",
        "    self.stock_dim = stock_dim\n",
        "    self.hmax = hmax\n",
        "    self.initial_amount = initial_amount\n",
        "    self.transaction_cost_pct =transaction_cost_pct\n",
        "    self.reward_scaling = reward_scaling\n",
        "    self.state_space = state_space\n",
        "    self.action_dim = action_space\n",
        "    self.tech_indicator_list = tech_indicator_list\n",
        "    self.possible_agents = TYPE_TO_NAMES[algo_type]\n",
        "    \n",
        "    # buy/sell ratio reference, to explore\n",
        "    self.end_day = len(self.df.index.unique()) - 1\n",
        "    self.stock_volume_reference = 10000\n",
        "\n",
        "    # spaces\n",
        "    # check: spaces for observations only? sharing will affect?\n",
        "    self.action_space = gym.spaces.Box(low = -1, high = 1, shape = (self.action_dim,))\n",
        "    self.observation_space = gym.spaces.Box(low=0, high=np.inf, shape = (1+len(self.tech_indicator_list), self.state_space))\n",
        "\n",
        "    # agents\n",
        "    self.agent_name_mapping = {\n",
        "        agent: AGENT_STR_TO_OBJECT[algo_type](policy, self, self.end_day+1) for agent in self.possible_agents\n",
        "    }\n",
        "    self.training_agent = None\n",
        "    self.day = {\n",
        "        agent: day for agent in self.possible_agents\n",
        "    }\n",
        "\n",
        "    # data\n",
        "    self.data = {\n",
        "        agent: self.df.loc[self.day[agent],:] for agent in self.possible_agents\n",
        "    }\n",
        "    self.covs = {\n",
        "        agent: [[x[0][0] for x in self.data[agent]['cov_list']]] for agent in self.possible_agents\n",
        "    }\n",
        "    self.state = {\n",
        "        agent: np.append(np.array(self.covs[agent]), [self.data[agent][tech].values.tolist() for tech in self.tech_indicator_list ], axis=0) for agent in self.possible_agents\n",
        "    }\n",
        "    self.terminal = False     \n",
        "    self.turbulence_threshold = turbulence_threshold   \n",
        "\n",
        "    # memory\n",
        "    self.portfolio_value = {\n",
        "        agent: self.initial_amount for agent in self.possible_agents\n",
        "    }\n",
        "    self.asset_memory = {\n",
        "        agent: [self.initial_amount] for agent in self.possible_agents\n",
        "    }\n",
        "    self.portfolio_return_memory = {\n",
        "        agent: [0] for agent in self.possible_agents\n",
        "    }\n",
        "    self.cum_portfolio_return_memory = {\n",
        "        agent: [0] for agent in self.possible_agents\n",
        "    }\n",
        "\n",
        "    # stock ratio\n",
        "    self.actions_memory = {\n",
        "        agent: [[0]*self.stock_dim] for agent in self.possible_agents\n",
        "    }\n",
        "    self.date_memory = {\n",
        "        agent: [self.data[agent][\"Date\"].unique()[0]] for agent in self.possible_agents\n",
        "    }\n",
        "\n",
        "    # free cash\n",
        "    self.money_memory = {\n",
        "        agent: [self.initial_amount] for agent in self.possible_agents\n",
        "    }\n",
        "    # cash + stock value\n",
        "    self.total_value_memory = {\n",
        "        agent: [self.initial_amount] for agent in self.possible_agents\n",
        "    }\n",
        "    # individual actions collection\n",
        "    self.individual_preds = {\n",
        "        agent: [] for agent in self.possible_agents\n",
        "    }\n",
        "\n",
        "    # render mode\n",
        "    self.render_mode = render_mode\n",
        "\n",
        "  def collect_individual_preds(self):\n",
        "    self.individual_preds = {\n",
        "        agent: [] for agent in self.possible_agents\n",
        "    }\n",
        "    for i in range(self.end_day+1):\n",
        "      # states are somewhat static\n",
        "      curr_data = self.df.loc[i,:]\n",
        "      curr_covs = [[x[0][0] for x in curr_data['cov_list']]]\n",
        "      curr_state = np.append(np.array(curr_covs), [curr_data[tech].values.tolist() for tech in self.tech_indicator_list ], axis=0)\n",
        "      for agent in self.possible_agents:\n",
        "        action, _states = self.agent_name_mapping[agent].predict(curr_state)\n",
        "        self.individual_preds[agent].append(action)\n",
        "  \n",
        "  def step(self, actions):\n",
        "\n",
        "    # termination\n",
        "    self.terminal = self.day[self.training_agent] >= self.end_day\n",
        "\n",
        "    if self.terminal:\n",
        "\n",
        "      # uncomment to print metrics on terminal\n",
        "      # print(\"=================================\")\n",
        "      # print(\"begin_money:{}\".format(self.asset_memory[self.training_agent][0]))           \n",
        "      # print(\"end_daily_total_portfolio_value:{}\".format(self.total_value_memory[self.training_agent][-1]))\n",
        "      # print(\"end_daily_portfolio_return:{}\".format(self.portfolio_return_memory[self.training_agent][-1]))\n",
        "      # print(\"end_cumulative_portfolio_return:{}\".format(self.cum_portfolio_return_memory[self.training_agent][-1]))\n",
        "\n",
        "      # df_daily_return = pd.DataFrame(self.portfolio_return_memory[self.training_agent])\n",
        "      # df_daily_return.columns = ['daily_return']\n",
        "      # if df_daily_return['daily_return'].std() !=0:\n",
        "      #   sharpe = (252**0.5)*df_daily_return['daily_return'].mean() / df_daily_return['daily_return'].std()\n",
        "      #   print(\"Sharpe: \",sharpe)\n",
        "      # print(\"=================================\")\n",
        "\n",
        "      return self.state[self.training_agent], self.reward[self.training_agent], self.terminal, {}\n",
        "\n",
        "    else:\n",
        "      # loop through all agents so that each of them predict an action (portfolio weights)\n",
        "      for agent in self.possible_agents:\n",
        "        # get action\n",
        "        if agent == self.training_agent:\n",
        "          action = actions\n",
        "        else:\n",
        "          # action, _states = self.agent_name_mapping[agent].predict(self.state[agent], deterministic=False)\n",
        "          action = self.individual_preds[agent][self.day[agent]]\n",
        "\n",
        "        # normalisation\n",
        "        weights = self.softmax_normalization(action) \n",
        "\n",
        "        # stock ratio - buy/sell/hold\n",
        "        prev_stock_ratio = self.actions_memory[agent][-1]\n",
        "        diff_stock_ratio = prev_stock_ratio - weights\n",
        "\n",
        "        # money - increase if sell, decrease if buy, no changes if hold\n",
        "        prev_money = self.money_memory[agent][-1]\n",
        "        curr_money = prev_money + sum(diff_stock_ratio * self.stock_volume_reference * self.data[agent][\"Close\"].values)\n",
        "        self.money_memory[agent].append(curr_money)\n",
        "\n",
        "        # total value - money + currently held stock value\n",
        "        curr_total = curr_money + sum(weights * self.stock_volume_reference * self.data[agent][\"Close\"].values)\n",
        "        self.total_value_memory[agent].append(curr_total)\n",
        "\n",
        "        # actions memory\n",
        "        self.actions_memory[agent].append(weights)\n",
        "        last_day_memory = self.data[agent]\n",
        "\n",
        "        # load next state\n",
        "        self.day[agent] += 1\n",
        "        self.data[agent] = self.df.loc[self.day[agent],:]\n",
        "        self.covs[agent] = [[x[0][0] for x in self.data[agent]['cov_list']]]\n",
        "        self.state[agent] =  np.append(np.array(self.covs[agent]), [self.data[agent][tech].values.tolist() for tech in self.tech_indicator_list ], axis=0)\n",
        "        \n",
        "        # calculate portfolio return\n",
        "        # individual stocks' return * weight\n",
        "        portfolio_return = sum(((self.data[agent][\"Close\"].values / last_day_memory[\"Close\"].values)-1)*weights)\n",
        "\n",
        "        # save into memory\n",
        "        self.portfolio_return_memory[agent].append(portfolio_return)\n",
        "        self.cum_portfolio_return_memory[agent].append(self.cum_portfolio_return_memory[agent][-1] + portfolio_return)\n",
        "        self.date_memory[agent].append(self.data[agent][\"Date\"].unique()[0])            \n",
        "        self.asset_memory[agent].append(curr_total)\n",
        "\n",
        "        # the reward is the new portfolio value or end portfolio value\n",
        "        self.reward[agent] = curr_total \n",
        "        #self.reward = self.reward*self.reward_scaling\n",
        "      \n",
        "      # penalise or reward the target agent based on the result of all other agents\n",
        "      all_rewards = list(self.reward.values())\n",
        "      for agent in self.possible_agents:\n",
        "        # ratio = smaller reward / greater reward\n",
        "        # rewards have to be similar across all agents\n",
        "        # the greater the difference, the greater the penalty\n",
        "        self.reward[agent] *= prod([\n",
        "            pv / self.reward[agent]\n",
        "            if self.reward[agent] > pv\n",
        "            else self.reward[agent] / pv\n",
        "            for pv in all_rewards\n",
        "        ])\n",
        "\n",
        "        # if money on hand is negative, large penalty is applied as this is unwanted\n",
        "        if self.money_memory[agent][-1] < 0:\n",
        "          self.reward[agent] *= -1\n",
        "\n",
        "    return self.state[agent], self.reward[self.training_agent], self.terminal, {}\n",
        "\n",
        "  def reset(self, seed=None, return_info=False, options=None):\n",
        "    # print(\"reset\")\n",
        "\n",
        "    # agents\n",
        "    self.agents = self.possible_agents[:]\n",
        "\n",
        "    # attributes\n",
        "    self.day = {\n",
        "        agent: 0 for agent in self.possible_agents\n",
        "    }\n",
        "    self.data = {\n",
        "        agent: self.df.loc[self.day[agent],:] for agent in self.possible_agents\n",
        "    }\n",
        "    self.covs = {\n",
        "        agent: [[x[0][0] for x in self.data[agent]['cov_list']]] for agent in self.possible_agents\n",
        "    }\n",
        "    self.state = {\n",
        "        agent: np.append(np.array(self.covs[agent]), [self.data[agent][tech].values.tolist() for tech in self.tech_indicator_list ], axis=0) for agent in self.possible_agents\n",
        "    }\n",
        "\n",
        "    # memory\n",
        "    self.portfolio_value = {\n",
        "        agent: self.initial_amount for agent in self.possible_agents\n",
        "    }\n",
        "    self.asset_memory = {\n",
        "        agent: [self.initial_amount] for agent in self.possible_agents\n",
        "    }\n",
        "    self.portfolio_return_memory = {\n",
        "        agent: [0] for agent in self.possible_agents\n",
        "    }\n",
        "    self.cum_portfolio_return_memory = {\n",
        "        agent: [0] for agent in self.possible_agents\n",
        "    }\n",
        "\n",
        "    # stock ratio\n",
        "    self.actions_memory = {\n",
        "        agent: [[0]*self.stock_dim] for agent in self.possible_agents\n",
        "    }\n",
        "    self.date_memory = {\n",
        "        agent: [self.data[agent][\"Date\"].unique()[0]] for agent in self.possible_agents\n",
        "    }\n",
        "\n",
        "    # free cash\n",
        "    self.money_memory = {\n",
        "        agent: [self.initial_amount] for agent in self.possible_agents\n",
        "    }\n",
        "    # cash + stock value\n",
        "    self.total_value_memory = {\n",
        "        agent: [self.initial_amount] for agent in self.possible_agents\n",
        "    }\n",
        "\n",
        "    # rewards\n",
        "    self.reward = {\n",
        "        agent: None for agent in self.possible_agents\n",
        "    }\n",
        "    \n",
        "    # misc\n",
        "    self.terminal = False \n",
        "    #self.cost = 0\n",
        "    #self.trades = 0\n",
        "    \n",
        "    return self.state[self.training_agent] \n",
        "\n",
        "  def render(self):\n",
        "    return self.state[self.training_agent]\n",
        "\n",
        "  def seed(self, seed=None):\n",
        "    self.np_random, seed = seeding.np_random(seed)\n",
        "    return [seed]\n",
        "  \n",
        "  def softmax_normalization(self, actions):\n",
        "    numerator = np.exp(actions)\n",
        "    denominator = np.sum(np.exp(actions))\n",
        "    softmax_output = numerator/denominator\n",
        "    return softmax_output\n",
        "\n",
        "  def set_training_agent(self, agent):\n",
        "    # print(agent)\n",
        "    self.training_agent = agent\n",
        "\n",
        "  def learn(self, total_timesteps=1000):\n",
        "    init_pv = {}\n",
        "    final_pv = {}\n",
        "    init_cash = {}\n",
        "    final_cash = {}\n",
        "    init_daily_pr = {}\n",
        "    final_daily_pr = {}\n",
        "    init_cum_pr = {}\n",
        "    final_cum_pr = {}\n",
        "    sharpe_ratio = {\n",
        "        agent: [] for agent in self.possible_agents\n",
        "    }\n",
        "\n",
        "    # run till terminal in each timestep\n",
        "    for n in range(total_timesteps):\n",
        "      print(\"Step:\", n+1)\n",
        "      self.collect_individual_preds()\n",
        "      for agent in self.possible_agents:\n",
        "        self.set_training_agent(agent)\n",
        "        self.agent_name_mapping[agent] = self.agent_name_mapping[agent].learn(total_timesteps=1)\n",
        "        sharpe_ratio[agent].append(calculate_sharpe(self.portfolio_return_memory[agent]))\n",
        "        \n",
        "      if n == 0:\n",
        "        # save init for plot\n",
        "        for agent in self.possible_agents:\n",
        "          init_pv[agent] = self.total_value_memory[agent]\n",
        "          init_daily_pr[agent] = self.portfolio_return_memory[agent]\n",
        "          init_cum_pr[agent] = self.cum_portfolio_return_memory[agent]\n",
        "          init_cash[agent] = self.money_memory[agent]\n",
        "    \n",
        "    # save final for plot\n",
        "    for agent in self.possible_agents:\n",
        "      final_pv[agent] = self.total_value_memory[agent]\n",
        "      final_daily_pr[agent] = self.portfolio_return_memory[agent]\n",
        "      final_cum_pr[agent] = self.cum_portfolio_return_memory[agent]\n",
        "      final_cash[agent] = self.money_memory[agent]\n",
        "    \n",
        "    return init_pv, final_pv, init_cash, final_cash, init_daily_pr, final_daily_pr, init_cum_pr, final_cum_pr, sharpe_ratio\n",
        "  \n",
        "  def save_weights(self):\n",
        "    for idx, agent in enumerate(self.possible_agents):\n",
        "      self.agent_name_mapping[agent].save(\"coop_{}_{}.zip\".format(agent, training_timesteps))\n",
        "\n",
        "  def load_weights(self):\n",
        "    for idx, agent in enumerate(self.possible_agents):\n",
        "      self.agent_name_mapping[agent].set_parameters(\"coop_{}_{}.zip\".format(agent, training_timesteps))\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### A2C"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s0JeqSBvsKGV",
        "outputId": "67234d8a-3b1e-453a-90b5-31aa3692b199"
      },
      "outputs": [],
      "source": [
        "a2c_coop_env = CooperativePMEnv(df=mixed_df, algo_type=\"A2C\", **env_kwargs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "(\n",
        "    a2c_coop_init_pv, \n",
        "    a2c_coop_final_pv, \n",
        "    a2c_coop_init_cash, \n",
        "    a2c_coop_final_cash, \n",
        "    a2c_coop_init_daily_pr, \n",
        "    a2c_coop_final_daily_pr, \n",
        "    a2c_coop_init_cum_pr, \n",
        "    a2c_coop_final_cum_pr, \n",
        "    a2c_coop_sr\n",
        ") = a2c_coop_env.learn(\n",
        "    total_timesteps=training_timesteps,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# save_to_json(\"a2c_coop_init_pv_{}.json\".format(training_timesteps), a2c_coop_init_pv)\n",
        "# save_to_json(\"a2c_coop_final_pv_{}.json\".format(training_timesteps), a2c_coop_final_pv)\n",
        "\n",
        "# save_to_json(\"a2c_coop_init_cash_{}.json\".format(training_timesteps), a2c_coop_init_cash)\n",
        "# save_to_json(\"a2c_coop_final_cash_{}.json\".format(training_timesteps), a2c_coop_final_cash)\n",
        "\n",
        "# save_to_json(\"a2c_coop_init_daily_pr_{}.json\".format(training_timesteps), a2c_coop_init_daily_pr)\n",
        "# save_to_json(\"a2c_coop_final_daily_pr_{}.json\".format(training_timesteps), a2c_coop_final_daily_pr)\n",
        "\n",
        "# save_to_json(\"a2c_coop_init_cum_pr_{}.json\".format(training_timesteps), a2c_coop_init_cum_pr)\n",
        "# save_to_json(\"a2c_coop_final_cum_pr_{}.json\".format(training_timesteps), a2c_coop_final_cum_pr)\n",
        "\n",
        "# save_to_json(\"a2c_coop_sr_{}.json\".format(training_timesteps), a2c_coop_sr)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Save weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "a2c_coop_env.save_weights()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load from pre-trained weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "a2c_load_coop = CooperativePMEnv(df=mixed_df, algo_type=\"A2C\", **env_kwargs)\n",
        "a2c_load_coop.load_weights()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### DDPG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ddpg_coop_env = CooperativePMEnv(df=mixed_df, algo_type=\"DDPG\", **env_kwargs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "(\n",
        "    ddpg_coop_init_pv, \n",
        "    ddpg_coop_final_pv, \n",
        "    ddpg_coop_init_cash, \n",
        "    ddpg_coop_final_cash, \n",
        "    ddpg_coop_init_daily_pr, \n",
        "    ddpg_coop_final_daily_pr, \n",
        "    ddpg_coop_init_cum_pr, \n",
        "    ddpg_coop_final_cum_pr, \n",
        "    ddpg_coop_sr\n",
        ") = ddpg_coop_env.learn(\n",
        "    total_timesteps=training_timesteps,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# save_to_json(\"ddpg_coop_init_pv_{}.json\".format(training_timesteps), ddpg_coop_init_pv)\n",
        "# save_to_json(\"ddpg_coop_final_pv_{}.json\".format(training_timesteps), ddpg_coop_final_pv)\n",
        "\n",
        "# save_to_json(\"ddpg_coop_init_cash_{}.json\".format(training_timesteps), ddpg_coop_init_cash)\n",
        "# save_to_json(\"ddpg_coop_final_cash_{}.json\".format(training_timesteps), ddpg_coop_final_cash)\n",
        "\n",
        "# save_to_json(\"ddpg_coop_init_daily_pr_{}.json\".format(training_timesteps), ddpg_coop_init_daily_pr)\n",
        "# save_to_json(\"ddpg_coop_final_daily_pr_{}.json\".format(training_timesteps), ddpg_coop_final_daily_pr)\n",
        "\n",
        "# save_to_json(\"ddpg_coop_init_cum_pr_{}.json\".format(training_timesteps), ddpg_coop_init_cum_pr)\n",
        "# save_to_json(\"ddpg_coop_final_cum_pr_{}.json\".format(training_timesteps), ddpg_coop_final_cum_pr)\n",
        "\n",
        "# save_to_json(\"ddpg_coop_sr_{}.json\".format(training_timesteps), ddpg_coop_sr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Save weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ddpg_coop_env.save_weights()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load from pre-trained weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ddpg_load_coop = CooperativePMEnv(df=mixed_df, algo_type=\"DDPG\", **env_kwargs)\n",
        "ddpg_load_coop.load_weights()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### PPO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ppo_coop_env = CooperativePMEnv(df=mixed_df, algo_type=\"PPO\", **env_kwargs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "(\n",
        "    ppo_coop_init_pv, \n",
        "    ppo_coop_final_pv, \n",
        "    ppo_coop_init_cash, \n",
        "    ppo_coop_final_cash, \n",
        "    ppo_coop_init_daily_pr, \n",
        "    ppo_coop_final_daily_pr, \n",
        "    ppo_coop_init_cum_pr, \n",
        "    ppo_coop_final_cum_pr, \n",
        "    ppo_coop_sr\n",
        ") = ppo_coop_env.learn(\n",
        "    total_timesteps=training_timesteps,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# save_to_json(\"ppo_coop_init_pv_{}.json\".format(training_timesteps), ppo_coop_init_pv)\n",
        "# save_to_json(\"ppo_coop_final_pv_{}.json\".format(training_timesteps), ppo_coop_final_pv)\n",
        "\n",
        "# save_to_json(\"ppo_coop_init_cash_{}.json\".format(training_timesteps), ppo_coop_init_cash)\n",
        "# save_to_json(\"ppo_coop_final_cash_{}.json\".format(training_timesteps), ppo_coop_final_cash)\n",
        "\n",
        "# save_to_json(\"ppo_coop_init_daily_pr_{}.json\".format(training_timesteps), ppo_coop_init_daily_pr)\n",
        "# save_to_json(\"ppo_coop_final_daily_pr_{}.json\".format(training_timesteps), ppo_coop_final_daily_pr)\n",
        "\n",
        "# save_to_json(\"ppo_coop_init_cum_pr_{}.json\".format(training_timesteps), ppo_coop_init_cum_pr)\n",
        "# save_to_json(\"ppo_coop_final_cum_pr_{}.json\".format(training_timesteps), ppo_coop_final_cum_pr)\n",
        "\n",
        "# save_to_json(\"ppo_coop_sr_{}.json\".format(training_timesteps), ppo_coop_sr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Save weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ppo_coop_env.save_weights()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load from pre-trained weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ppo_load_coop = CooperativePMEnv(df=mixed_df, algo_type=\"PPO\", **env_kwargs)\n",
        "ppo_load_coop.load_weights()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Plots"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Market Trend"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.plot(stocks[\"AAPL\"][\"Close\"].index, stocks[\"AAPL\"][\"Close\"], label=\"AAPL\")\n",
        "plt.plot(stocks[\"TSLA\"][\"Close\"].index, stocks[\"TSLA\"][\"Close\"], label=\"TSLA\")\n",
        "plt.title(\"Daily Closing Value\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## A2C"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Competitive"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Portfolio Value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_line_graph(a2c_comp_env, a2c_comp_env.date_memory[a2c_comp_env.possible_agents[0]], a2c_comp_init_pv, \"Portfolio Value before Training\")\n",
        "plot_line_graph(a2c_comp_env, a2c_comp_env.date_memory[a2c_comp_env.possible_agents[0]], a2c_comp_final_pv, \"Portfolio Value after Training\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Cash Movement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_line_graph(a2c_comp_env, a2c_comp_env.date_memory[a2c_comp_env.possible_agents[0]], a2c_comp_init_cash, \"Cash Movement before Training\")\n",
        "plot_line_graph(a2c_comp_env, a2c_comp_env.date_memory[a2c_comp_env.possible_agents[0]], a2c_comp_final_cash, \"Cash Movement after Training\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Daily Portfolio Return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_line_graph(a2c_comp_env, a2c_comp_env.date_memory[a2c_comp_env.possible_agents[0]], a2c_comp_init_daily_pr, \"Daily Portfoliio Return before Training\")\n",
        "plot_line_graph(a2c_comp_env, a2c_comp_env.date_memory[a2c_comp_env.possible_agents[0]], a2c_comp_final_daily_pr, \"Daily Portfoliio Return after Training\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Cumulative Portfolio Return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_line_graph(a2c_comp_env, a2c_comp_env.date_memory[a2c_comp_env.possible_agents[0]], a2c_comp_init_cum_pr, \"Cumulative Portfoliio Return before Training\")\n",
        "plot_line_graph(a2c_comp_env, a2c_comp_env.date_memory[a2c_comp_env.possible_agents[0]], a2c_comp_final_cum_pr, \"Cumulative Portfoliio Return after Training\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sharpe Ratio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_line_graph(a2c_comp_env, range(1, training_timesteps+1), a2c_comp_sr, \"Competitive Sharpe Ratio Trend\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Cooperative"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Portfolio Value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_line_graph(a2c_coop_env, a2c_coop_env.date_memory[a2c_coop_env.possible_agents[0]], a2c_coop_init_pv, \"Portfolio Value before Training\")\n",
        "plot_line_graph(a2c_coop_env, a2c_coop_env.date_memory[a2c_coop_env.possible_agents[0]], a2c_coop_final_pv, \"Portfolio Value after Training\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Cash Movement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_line_graph(a2c_coop_env, a2c_coop_env.date_memory[a2c_coop_env.possible_agents[0]], a2c_coop_init_cash, \"Cash Movement before Training\")\n",
        "plot_line_graph(a2c_coop_env, a2c_coop_env.date_memory[a2c_coop_env.possible_agents[0]], a2c_coop_final_cash, \"Cash Movement after Training\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Daily Portfolio Return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_line_graph(a2c_coop_env, a2c_coop_env.date_memory[a2c_coop_env.possible_agents[0]], a2c_coop_init_daily_pr, \"Daily Portfoliio Return before Training\")\n",
        "plot_line_graph(a2c_coop_env, a2c_coop_env.date_memory[a2c_coop_env.possible_agents[0]], a2c_coop_final_daily_pr, \"Daily Portfoliio Return after Training\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Cumulative Portfolio Return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_line_graph(a2c_coop_env, a2c_coop_env.date_memory[a2c_coop_env.possible_agents[0]], a2c_coop_init_cum_pr, \"Cumulative Portfolio Return before Training\")\n",
        "plot_line_graph(a2c_coop_env, a2c_coop_env.date_memory[a2c_coop_env.possible_agents[0]], a2c_coop_final_cum_pr, \"Cumulative Portfolio Return after Training\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sharpe Ratio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_line_graph(a2c_coop_env, range(1, training_timesteps+1), a2c_coop_sr, \"Cooperative Sharpe Ratio Trend\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## PPO"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Competitive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Portfolio Value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_line_graph(ppo_comp_env, ppo_comp_env.date_memory[ppo_comp_env.possible_agents[0]], ppo_comp_init_pv, \"Portfolio Value before Training\")\n",
        "plot_line_graph(ppo_comp_env, ppo_comp_env.date_memory[ppo_comp_env.possible_agents[0]], ppo_comp_final_pv, \"Portfolio Value after Training\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Cash Movement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_line_graph(ppo_comp_env, ppo_comp_env.date_memory[ppo_comp_env.possible_agents[0]], ppo_comp_init_cash, \"Cash Movement before Training\")\n",
        "plot_line_graph(ppo_comp_env, ppo_comp_env.date_memory[ppo_comp_env.possible_agents[0]], ppo_comp_final_cash, \"Cash Movement after Training\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Daily Portfolio Return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_line_graph(ppo_comp_env, ppo_comp_env.date_memory[ppo_comp_env.possible_agents[0]], ppo_comp_init_daily_pr, \"Daily Portfolio Return before Training\")\n",
        "plot_line_graph(ppo_comp_env, ppo_comp_env.date_memory[ppo_comp_env.possible_agents[0]], ppo_comp_final_daily_pr, \"Daily Portfolio Return after Training\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Cumulative Portfolio Return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_line_graph(ppo_comp_env, ppo_comp_env.date_memory[ppo_comp_env.possible_agents[0]], ppo_comp_init_cum_pr, \"Cumulative Portfolio Return before Training\")\n",
        "plot_line_graph(ppo_comp_env, ppo_comp_env.date_memory[ppo_comp_env.possible_agents[0]], ppo_comp_final_cum_pr, \"Cumulative Portfolio Return after Training\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sharpe Ratio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_line_graph(ppo_comp_env, range(1, training_timesteps+1), ppo_comp_sr, \"Competitive Sharpe Ratio Trend\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Cooperative"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Portfolio Value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_line_graph(ppo_coop_env, ppo_coop_env.date_memory[ppo_coop_env.possible_agents[0]], ppo_coop_init_pv, \"Portfolio Value before Training\")\n",
        "plot_line_graph(ppo_coop_env, ppo_coop_env.date_memory[ppo_coop_env.possible_agents[0]], ppo_coop_final_pv, \"Portfolio Value after Training\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Cash Movement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_line_graph(ppo_coop_env, ppo_coop_env.date_memory[ppo_coop_env.possible_agents[0]], ppo_coop_init_cash, \"Cash Movement before Training\")\n",
        "plot_line_graph(ppo_coop_env, ppo_coop_env.date_memory[ppo_coop_env.possible_agents[0]], ppo_coop_final_cash, \"Cash Movement after Training\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Daily Portfolio Return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_line_graph(ppo_coop_env, ppo_coop_env.date_memory[ppo_coop_env.possible_agents[0]], ppo_coop_init_daily_pr, \"Daily Portfolio Return before Training\")\n",
        "plot_line_graph(ppo_coop_env, ppo_coop_env.date_memory[ppo_coop_env.possible_agents[0]], ppo_coop_final_daily_pr, \"Daily Portfolio Return after Training\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Cumulative Portfolio Return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_line_graph(ppo_coop_env, ppo_coop_env.date_memory[ppo_coop_env.possible_agents[0]], ppo_coop_init_cum_pr, \"Cumulative Portfolio Return before Training\")\n",
        "plot_line_graph(ppo_coop_env, ppo_coop_env.date_memory[ppo_coop_env.possible_agents[0]], ppo_coop_final_cum_pr, \"Cumulative Portfolio Return after Training\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sharpe Ratio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_line_graph(ppo_coop_env, range(1, training_timesteps+1), ppo_coop_sr, \"Cooperative Sharpe Ratio Trend\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## BenchMark\n",
        "After the strategies have been construct, we are going to use buy and hold and constance_rebalancing to compare the efficiency of the algorithm.\n",
        "We are using DRR, CRR, Varience, and Sharpe ratio to measure the performance of the benchmark algo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### DRR\n",
        "The Daily rate of return can be computed by\n",
        "\n",
        "$$ DRR = { Price_{today} - Price_{prevday}} $$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "# use DRR column\n",
        "def portfolio_daily_rate_of_return(portfolio_df, initial_amount):\n",
        "    if not ('DRR' in portfolio_df):\n",
        "        portfolio_df['DRR'] = np.nan\n",
        "    portfolio_df['DRR'] = ( portfolio_df['Sum'] - np.roll(portfolio_df['Sum'], shift=1))/portfolio_df['Sum']\n",
        "    portfolio_df.loc[portfolio_df.index[0], 'DRR']= (portfolio_df.loc[portfolio_df.index[0], 'Sum'] -initial_amount) /initial_amount\n",
        "    return portfolio_df \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### CRR\n",
        "The Cumulative rate or the sum of the DRR can be computed by\n",
        "\n",
        "$$ CRR = { Price_{today} - Price_{init}} $$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "# use CRR column\n",
        "def portfolio_cumulative_rate_of_return(portfolio_df, initial_amount):\n",
        "    if not ('DRR' in portfolio_df):\n",
        "        portfolio_df = portfolio_daily_rate_of_return(portfolio_df, initial_amount)\n",
        "    if not ('CRR' in portfolio_df):\n",
        "        portfolio_df['CRR'] = np.nan\n",
        "    \n",
        "    portfolio_df['CRR'] = ( portfolio_df['Sum'] - initial_amount)/initial_amount\n",
        "\n",
        "    return portfolio_df "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "def portfolio_get_cumulative_rate_of_return(portfolio_df, index=-1):\n",
        "    return portfolio_df.loc[portfolio_df.index[index], 'CRR']"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Varience\n",
        "The Varience can be computed by using the following equation\n",
        "\n",
        "$$ Var = { \\Sigma( DRR - E_{DRR} )^2 \\over Time} $$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "def portfolio_varience(portfolio_df):\n",
        "    drr_sum = portfolio_df.loc[portfolio_df.index[-1], 'CRR']/portfolio_df.__len__()\n",
        "    diff_sq = np.square(portfolio_df['DRR'] - drr_sum)\n",
        "    var = diff_sq.sum()/portfolio_df.__len__()\n",
        "    return var"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Sharpe ratio\n",
        "The ratio can be computed by using the following equation\n",
        "\n",
        "$$ Sharpe = { R_{portfolio} - R_{riskfree} \\over \\sigma_{portfolio}} $$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "# compute the portfolio sharpe ratio\n",
        "def portfolio_sharpe_ratio(portfolio_df, port_sd, riskfree=0.0151, at_index=-1):\n",
        "    year_count = portfolio_df.__len__()/252\n",
        "    asset_return = portfolio_df.loc[portfolio_df.index[at_index], 'CRR']/year_count\n",
        "    return (asset_return - riskfree)/port_sd"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define a class for running rebalancing algo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ConstanceBalancingOnTime:\n",
        "    def __init__(self, \n",
        "                df,\n",
        "                holding_ratio,\n",
        "                rebalance_time,\n",
        "                initial_amount,\n",
        "                transaction_cost_pct,\n",
        "                lookback=252,\n",
        "                day = 0):\n",
        "        self.df = df\n",
        "        self.rebalance_time = rebalance_time\n",
        "        self.holding_ratio = np.array(holding_ratio)\n",
        "        self.initial_amount = initial_amount\n",
        "        self.transaction_cost_pct = transaction_cost_pct\n",
        "        self.lookback = lookback\n",
        "        self.day = day\n",
        "\n",
        "        self.ticker_list = df[\"tic\"].unique()\n",
        "        self.stock_count = len(self.ticker_list)\n",
        "        # the first one is cash on hand followed by the stock in the tickers\n",
        "        self.asset_amount = self.holding_ratio * initial_amount\n",
        "\n",
        "        self.portfolio_memory = self._create_initial_memory()\n",
        "        self._write_memory_at_index(0)\n",
        "\n",
        "        #for keeping matric\n",
        "        self.metric = {}\n",
        "\n",
        "        \n",
        "\n",
        "    def step(self):\n",
        "        # incase \n",
        "        self.day += 1\n",
        "        # if (self.day == 1):\n",
        "        #     current_value = self.initial_amount / (self.transaction_cost_pct * self.holding_ratio[1:].sum() + self.initial_amount)\n",
        "        #     self.asset_amount = current_value * self.holding_ratio\n",
        "        #     return\n",
        "        \n",
        "        for index in range(self.stock_count):\n",
        "            try:\n",
        "                self.asset_amount[index+1] *= self._update_assetprice_by_ratio(int(self.day), self.ticker_list[index])\n",
        "            except:\n",
        "                print(\"error at day\",self.day, index)\n",
        "        \n",
        "        # rebalancing\n",
        "        if (self.day % self.rebalance_time == 0):\n",
        "\n",
        "            after_asset_amount = np.array([])\n",
        "            \n",
        "            current_asset_value = self.asset_amount.sum()\n",
        "            current_asset_ratio = self.asset_amount/current_asset_value\n",
        "\n",
        "            #check the sign of asset allowcation\n",
        "            asset_adapt_sign = self.holding_ratio >= current_asset_ratio\n",
        "                \n",
        "            asset_matrix = np.array([np.append(self.holding_ratio.copy(), 0)])\n",
        "            \n",
        "            for i in range(self.stock_count+1): \n",
        "                # the amount of add in that we will add into the asset line\n",
        "                add_in_asset_line = 1\n",
        "                if ((self.holding_ratio == current_asset_ratio)[i] ):\n",
        "                    pass\n",
        "                elif (asset_adapt_sign[i] ):\n",
        "                    add_in_asset_line += self.transaction_cost_pct\n",
        "                else:\n",
        "                    add_in_asset_line -= self.transaction_cost_pct\n",
        "                # print(add_in_asset_line)\n",
        "                current_asset_line = np.zeros(self.stock_count+2)\n",
        "                current_asset_line[i] = add_in_asset_line\n",
        "                current_asset_line[-1] = 1\n",
        "                # print(asset_matrix.shape, current_asset_line.shape)\n",
        "                # print(asset_matrix, current_asset_line)\n",
        "                asset_matrix = np.append(asset_matrix.copy(), np.array([current_asset_line.copy()]), axis=0)\n",
        "            asset_matrix = asset_matrix.T\n",
        "            \n",
        "            modify_amount = np.linalg.solve(asset_matrix, np.append(self.asset_amount, 0))[1:]\n",
        "            self.asset_amount = self.asset_amount - modify_amount\n",
        "        \n",
        "        # write the allocation to the memory df\n",
        "        self._write_memory_at_index(self.day)\n",
        "\n",
        "    def run(self):\n",
        "        time_range = self.df.__len__() / self.stock_count\n",
        "        for i in range(int(time_range)-1):\n",
        "            self.step()\n",
        "\n",
        "    def reset(self):\n",
        "        self.day = 0\n",
        "        self.asset_amount = self.holding_ratio * self.initial_amount\n",
        "\n",
        "        self.portfolio_memory = self._create_initial_memory()\n",
        "        self._write_memory_at_index(0)\n",
        "\n",
        "    def get_asset_amount(self):\n",
        "        return self.asset_amount.sum()\n",
        "    \n",
        "    def add_return(self, method):\n",
        "        self.portfolio_memory = method(self.portfolio_memory, self.initial_amount)\n",
        "\n",
        "    def add_metric(self, new_matric, new_matric_name, **kwargs):\n",
        "        matric_res = new_matric(self.portfolio_memory, **kwargs)\n",
        "        self.metric.update({new_matric_name: matric_res}) \n",
        "\n",
        "    def _create_initial_memory(self):\n",
        "        col = np.append(np.array(['Cash']), self.ticker_list)\n",
        "        col = np.append(col, np.array(['Sum']))\n",
        "        initial_memory = pd.DataFrame(index=self.df.index.unique(), columns=col)\n",
        "        \n",
        "        return initial_memory\n",
        "    \n",
        "    def _update_assetprice_by_ratio(self, time_index: int, ticker: str):\n",
        "        return self.df[self.df['tic'] == ticker].iloc[time_index]['Close']/self.df[self.df['tic'] == ticker].iloc[time_index-1]['Close']\n",
        "    \n",
        "    def _write_memory_at_index(self, index: int):\n",
        "        self.portfolio_memory.iloc[index]['Cash'] = self.asset_amount[0]\n",
        "        \n",
        "        for stock_index in range(self.stock_count):\n",
        "            current_ticker = self.ticker_list[stock_index]\n",
        "            self.portfolio_memory.iloc[index][current_ticker] = self.asset_amount[stock_index+1]\n",
        "        self.portfolio_memory.iloc[index]['Sum'] = self.asset_amount.sum()\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array(['AAPL', 'CAAS'], dtype=object)"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mixed_df[\"tic\"].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "#df for testing\n",
        "test_df = mixed_df.copy().set_index('Date')\n",
        "risk_free_2022 = 0.025"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Single stock"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "env_kwargs = {\n",
        "  \"holding_ratio\": [0, 0.5], \n",
        "  \"rebalance_time\": 60,\n",
        "  \"initial_amount\": 1000000, \n",
        "  \"transaction_cost_pct\": 0.001\n",
        "}\n",
        "\n",
        "cr = ConstanceBalancingOnTime(df = new_aapl_df.copy().set_index('Date'), **env_kwargs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "cr.reset()\n",
        "cr.run()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "cr.add_return(portfolio_daily_rate_of_return)\n",
        "cr.add_return(portfolio_cumulative_rate_of_return)\n",
        "\n",
        "cr.add_metric(portfolio_varience, \"varience\")\n",
        "cr.add_metric(portfolio_get_cumulative_rate_of_return, \"crr\")\n",
        "env_kwargs = {\n",
        "  \"port_sd\": np.sqrt(cr.metric['varience']), \n",
        "  \"riskfree\": risk_free_2022,\n",
        "}\n",
        "\n",
        "cr.add_metric(portfolio_sharpe_ratio, \"sharpe\", **env_kwargs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "rebalance time 60\n",
            "initial amount 1000000\n",
            "tickers ['AAPL']\n",
            "stock count 1\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'varience': 0.0007138874448694907,\n",
              " 'crr': 0.8275545459579413,\n",
              " 'sharpe': 6.815237131372679}"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(\"rebalance time\", cr.rebalance_time)\n",
        "print(\"initial amount\", cr.initial_amount)\n",
        "print(\"tickers\", cr.ticker_list)\n",
        "print(\"stock count\", cr.stock_count)\n",
        "\n",
        "cr.metric"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Constance rebalance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "env_kwargs = {\n",
        "  \"holding_ratio\": [0,0.5, 0.5], \n",
        "  \"rebalance_time\": 60,\n",
        "  \"initial_amount\": 1000000, \n",
        "  \"transaction_cost_pct\": 0.001\n",
        "}\n",
        "\n",
        "cr = ConstanceBalancingOnTime(df = test_df, **env_kwargs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "cr.reset()\n",
        "cr.run()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "cr.add_return(portfolio_daily_rate_of_return)\n",
        "cr.add_return(portfolio_cumulative_rate_of_return)\n",
        "\n",
        "cr.add_metric(portfolio_varience, \"varience\")\n",
        "cr.add_metric(portfolio_get_cumulative_rate_of_return, \"crr\")\n",
        "env_kwargs = {\n",
        "  \"port_sd\": np.sqrt(cr.metric['varience']), \n",
        "  \"riskfree\": risk_free_2022,\n",
        "}\n",
        "\n",
        "cr.add_metric(portfolio_sharpe_ratio, \"sharpe\", **env_kwargs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "rebalance time 60\n",
            "initial amount 1000000\n",
            "tickers ['AAPL' 'CAAS']\n",
            "stock count 2\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'varience': 0.0010014236330381694,\n",
              " 'crr': 2.60775359627078,\n",
              " 'sharpe': 19.831894080392228}"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(\"rebalance time\", cr.rebalance_time)\n",
        "print(\"initial amount\", cr.initial_amount)\n",
        "print(\"tickers\", cr.ticker_list)\n",
        "print(\"stock count\", cr.stock_count)\n",
        "\n",
        "cr.metric"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Buy and Hold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "## let the time to be very high so the portfolio will not rebalance\n",
        "env_kwargs = {\n",
        "  \"holding_ratio\": [0,0.5,0.5], \n",
        "  \"rebalance_time\": 9999999999,\n",
        "  \"initial_amount\": 1000000, \n",
        "  \"transaction_cost_pct\": 0.001\n",
        "}\n",
        "\n",
        "buyhold = ConstanceBalancingOnTime(df = test_df, **env_kwargs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [],
      "source": [
        "buyhold.reset()\n",
        "buyhold.run()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "buyhold.add_return(portfolio_daily_rate_of_return)\n",
        "buyhold.add_return(portfolio_cumulative_rate_of_return)\n",
        "buyhold.add_metric(portfolio_varience, \"varience\")\n",
        "buyhold.add_metric(portfolio_get_cumulative_rate_of_return, \"crr\")\n",
        "env_kwargs = {\n",
        "  \"port_sd\": np.sqrt(cr.metric['varience']), \n",
        "  \"riskfree\": risk_free_2022,\n",
        "}\n",
        "\n",
        "buyhold.add_metric(portfolio_sharpe_ratio, \"sharpe\", **env_kwargs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "rebalance time 9999999999\n",
            "initial amount 1000000\n",
            "tickers ['AAPL' 'CAAS']\n",
            "stock count 2\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'varience': 0.0006593813320346426,\n",
              " 'crr': 1.9875545841049216,\n",
              " 'sharpe': 14.927411207558738}"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(\"rebalance time\", buyhold.rebalance_time)\n",
        "print(\"initial amount\", buyhold.initial_amount)\n",
        "print(\"tickers\", buyhold.ticker_list)\n",
        "print(\"stock count\", buyhold.stock_count)\n",
        "\n",
        "buyhold.metric\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
