{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/interritus141/COMP0031-Group-Research-Project/blob/master/COMP0031.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ruZOtW7WfjEi"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import gym\n",
        "from stable_baselines3 import A2C, DDPG, DQN, PPO\n",
        "from ta import add_all_ta_features\n",
        "import yfinance as yf\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "import functools\n",
        "import os\n",
        "import csv\n",
        "import json\n",
        "from math import prod"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Helpers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_line_graph(env, x_vals, y_dict, title):\n",
        "  for agent in env.possible_agents:\n",
        "    plt.plot(x_vals, y_dict[agent], label=agent)\n",
        "  plt.title(title)\n",
        "  plt.legend()\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_sharpe(portfolio_return_mem):\n",
        "  df_daily_return = pd.DataFrame(portfolio_return_mem)\n",
        "  df_daily_return.columns = ['daily_return']\n",
        "  sharpe = -1 \n",
        "  if df_daily_return['daily_return'].std() != 0:\n",
        "    sharpe = (252**0.5)*df_daily_return['daily_return'].mean() / df_daily_return['daily_return'].std()\n",
        "  return sharpe # -1 means error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def save_to_json(filename, data_dict):\n",
        "  with open(filename, \"w\") as data_out:\n",
        "    json.dump(data_dict, data_out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "itPimO3WZcN_"
      },
      "source": [
        "# Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NdcDd-Iqd_-Z"
      },
      "source": [
        "## Tech Indicators"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lbA-Y5Y3grjA"
      },
      "outputs": [],
      "source": [
        "def add_ta(df):\n",
        "  ta_df = add_all_ta_features(df, open=\"Open\", high=\"High\", low=\"Low\", close=\"Close\", volume=\"Volume\")\n",
        "  # print(ta_df.columns)\n",
        "  ta_df = ta_df[[\"Open\", \"High\", \"Low\", \"Close\", \"Volume\", \"Adj Close\", \"volume_obv\",\n",
        "                                \"volume_adi\", \"trend_adx\", \"momentum_ao\", \"trend_macd\", \"momentum_rsi\", \n",
        "                                \"momentum_stoch\"]]\n",
        "  ta_df = ta_df.fillna(ta_df.mean())\n",
        "  return ta_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ttJNGUWuvVF2"
      },
      "source": [
        "## Stocks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3JCv_b7iw9b"
      },
      "source": [
        "1. Apple Inc. (AAPL)\n",
        "2. Microsoft Corp. (MSFT)\n",
        "3. Amazon.com, Inc. ( AMZN)\n",
        "4. Tesla, Inc. (TSLA)\n",
        "5. Nvidia Corp. (NVDA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KQW7POdeiKTJ",
        "outputId": "ab402ff2-b8a2-4834-a616-e8484aba3b1c"
      },
      "outputs": [],
      "source": [
        "# interval = 1m,2m,5m,15m,30m,60m,90m,1h,1d,5d,1wk,1mo,3mo\n",
        "# prepost = T/F\n",
        "\n",
        "stock_volatilities = {}\n",
        "stocks = dict.fromkeys([\"AAPL\", \"MSFT\", \"AMZN\", \"TSLA\", \"NVDA\", \"CAAS\"])\n",
        "\n",
        "for stock in stocks.keys():\n",
        "  stock_df = yf.download(stock, start=\"2018-01-01\", end=\"2022-12-31\", keepna=True)\n",
        "  stock_df = stock_df.fillna(stock_df.mean())\n",
        "  stock_df = add_ta(stock_df)\n",
        "  stocks[stock] = stock_df\n",
        "  stock_volatilities[stock] = None"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Volatility"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def add_volatility(df, df_name):\n",
        "  df[\"Log returns\"] = np.log(df['Close'] / df['Close'].shift())\n",
        "  stock_volatilities[df_name] = df['Log returns'].std() * 252 ** .5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def visualise_volatility(df, df_name, volatility):\n",
        "  fig, ax = plt.subplots()\n",
        "  df['Log returns'].hist(ax=ax, bins=50, alpha=0.6, color='b')\n",
        "  ax.set_xlabel(\"Log return\")\n",
        "  ax.set_ylabel(\"Freq of log return\")\n",
        "  ax.set_title(\"{:s} volatility: {:.2f}%\".format(df_name, volatility*100))\n",
        "\n",
        "for stock, stock_df in stocks.items():\n",
        "  add_volatility(stock_df, stock)\n",
        "  # visualise_volatility(stock_df, stock, stock_volatilities[stock]) # use to generate graphs\n",
        "\n",
        "print(stock_volatilities)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cov"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uJtS7XLJuSs2"
      },
      "outputs": [],
      "source": [
        "def add_cov(df):\n",
        "  df = df.reset_index()\n",
        "\n",
        "  cov_list = []\n",
        "  return_list = []\n",
        "\n",
        "  # look back is one year\n",
        "  lookback=252\n",
        "  for i in range(lookback,len(df.index.unique())):\n",
        "    data_lookback = df.iloc[i-lookback:i,:]\n",
        "    price_lookback=data_lookback.pivot_table(index = 'Date', values = 'Close')\n",
        "    return_lookback = price_lookback.pct_change().dropna()\n",
        "    return_list.append(return_lookback)\n",
        "\n",
        "    covs = return_lookback.cov().values \n",
        "    cov_list.append(covs)\n",
        "\n",
        "\n",
        "  df_cov = pd.DataFrame({'Date':df[\"Date\"].unique()[lookback:],'cov_list':cov_list,'return_list':return_list})\n",
        "  df = df.merge(df_cov, on='Date')\n",
        "  df = df.sort_values(['Date']).reset_index(drop=True)\n",
        "  return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QwlliL4VulGv"
      },
      "outputs": [],
      "source": [
        "# high volatility\n",
        "new_aapl_df = add_cov(stocks[\"AAPL\"])\n",
        "\n",
        "# low volatility\n",
        "new_tsla_df = add_cov(stocks[\"TSLA\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8OADGaYLI-bv"
      },
      "outputs": [],
      "source": [
        "data_aapl_df = new_aapl_df.copy()\n",
        "data_aapl_df[\"tic\"] = \"AAPL\"\n",
        "\n",
        "data_tsla_df = new_tsla_df.copy()\n",
        "data_tsla_df[\"tic\"] = \"TSLA\"\n",
        "\n",
        "mixed_df = pd.concat([data_aapl_df, data_tsla_df])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mVYjY6QFj2Uy"
      },
      "source": [
        "# Environment"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XIA9Tg7bmZGN"
      },
      "outputs": [],
      "source": [
        "policy = \"MlpPolicy\"\n",
        "training_timesteps = 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "AGENT_STR_TO_OBJECT = {\n",
        "    \"A2C\": A2C,\n",
        "    \"DDPG\": DDPG,\n",
        "    \"PPO\": PPO,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "TYPE_TO_NAMES = {\n",
        "    \"A2C\": [\"A2C1\", \"A2C2\", \"A2C3\"],\n",
        "    \"DDPG\": [\"DDPG1\", \"DDPG2\", \"DDPG3\"],\n",
        "    \"PPO\": [\"PPO1\", \"PPO2\", \"PPO3\"],\n",
        "    \"Mixed\": [\"A2C\", \"DDPG\", \"PPO\"],\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# configurations\n",
        "\n",
        "stock_dimension = len(mixed_df[\"tic\"].unique())\n",
        "state_space = 2\n",
        "\n",
        "env_kwargs = {\n",
        "  \"hmax\": 100, \n",
        "  \"initial_amount\": 1000000, \n",
        "  \"transaction_cost_pct\": 0.001, \n",
        "  \"state_space\": state_space, \n",
        "  \"stock_dim\": stock_dimension, \n",
        "  \"tech_indicator_list\": [\n",
        "    \"volume_obv\",\n",
        "    \"volume_adi\", \n",
        "    \"trend_adx\", \n",
        "    \"momentum_ao\", \n",
        "    \"trend_macd\", \n",
        "    \"momentum_rsi\", \n",
        "    \"momentum_stoch\"\n",
        "  ], \n",
        "  \"action_space\": stock_dimension, \n",
        "  \"reward_scaling\": 1e-4,\n",
        "    \n",
        "}"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Competitive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CompetitivePMEnv(gym.Env):\n",
        "  metadata = {\"render_modes\": [\"human\"], \"name\": \"marlpm_v1\"}\n",
        "\n",
        "  def __init__(\n",
        "      self, \n",
        "      df,\n",
        "      stock_dim,\n",
        "      hmax,\n",
        "      initial_amount,\n",
        "      transaction_cost_pct,\n",
        "      reward_scaling,\n",
        "      state_space,\n",
        "      action_space,\n",
        "      tech_indicator_list,\n",
        "      turbulence_threshold=None,\n",
        "      lookback=252,\n",
        "      day=0,\n",
        "      render_mode=None,\n",
        "      algo_type=\"Mixed\", # default=mixed\n",
        "  ):\n",
        "\n",
        "    assert algo_type in [\"A2C\", \"DDPG\", \"PPO\", \"Mixed\"]\n",
        "\n",
        "    # attributes\n",
        "    self.lookback=lookback\n",
        "    self.df = df\n",
        "    self.stock_dim = stock_dim\n",
        "    self.hmax = hmax\n",
        "    self.initial_amount = initial_amount\n",
        "    self.transaction_cost_pct =transaction_cost_pct\n",
        "    self.reward_scaling = reward_scaling\n",
        "    self.state_space = state_space\n",
        "    self.action_dim = action_space\n",
        "    self.tech_indicator_list = tech_indicator_list\n",
        "    self.possible_agents = TYPE_TO_NAMES[algo_type]\n",
        "    \n",
        "    # buy/sell ratio reference, to explore\n",
        "    self.end_day = len(self.df.index.unique()) - 1\n",
        "    self.stock_volume_reference = 10000\n",
        "\n",
        "    # spaces\n",
        "    # check: spaces for observations only? sharing will affect?\n",
        "    self.action_space = gym.spaces.Box(low = -1, high = 1, shape = (self.action_dim,))\n",
        "    self.observation_space = gym.spaces.Box(low=0, high=np.inf, shape = (1+len(self.tech_indicator_list), self.state_space))\n",
        "\n",
        "    # agents\n",
        "    self.agent_name_mapping = {\n",
        "        # agent: AGENT_STR_TO_OBJECT[agent](policy, self) for agent in self.possible_agents\n",
        "        agent: AGENT_STR_TO_OBJECT[algo_type](policy, self, n_steps=self.end_day) for agent in self.possible_agents\n",
        "    }\n",
        "    self.training_agent = None\n",
        "    self.day = {\n",
        "        agent: day for agent in self.possible_agents\n",
        "    }\n",
        "\n",
        "    # data\n",
        "    self.data = {\n",
        "        agent: self.df.loc[self.day[agent],:] for agent in self.possible_agents\n",
        "    }\n",
        "    self.covs = {\n",
        "        agent: [[x[0][0] for x in self.data[agent]['cov_list']]] for agent in self.possible_agents\n",
        "    }\n",
        "    self.state = {\n",
        "        agent: np.append(np.array(self.covs[agent]), [self.data[agent][tech].values.tolist() for tech in self.tech_indicator_list ], axis=0) for agent in self.possible_agents\n",
        "    }\n",
        "    self.terminal = False     \n",
        "    self.turbulence_threshold = turbulence_threshold   \n",
        "\n",
        "    # memory\n",
        "    self.portfolio_value = {\n",
        "        agent: self.initial_amount for agent in self.possible_agents\n",
        "    }\n",
        "    self.asset_memory = {\n",
        "        agent: [self.initial_amount] for agent in self.possible_agents\n",
        "    }\n",
        "    self.portfolio_return_memory = {\n",
        "        agent: [0] for agent in self.possible_agents\n",
        "    }\n",
        "    self.cum_portfolio_return_memory = {\n",
        "        agent: [0] for agent in self.possible_agents\n",
        "    }\n",
        "\n",
        "    # stock ratio\n",
        "    self.actions_memory = {\n",
        "        agent: [[0]*self.stock_dim] for agent in self.possible_agents\n",
        "    }\n",
        "    self.date_memory = {\n",
        "        agent: [self.data[agent][\"Date\"].unique()[0]] for agent in self.possible_agents\n",
        "    }\n",
        "\n",
        "    # free cash\n",
        "    self.money_memory = {\n",
        "        agent: [self.initial_amount] for agent in self.possible_agents\n",
        "    }\n",
        "    # cash + stock value\n",
        "    self.total_value_memory = {\n",
        "        agent: [self.initial_amount] for agent in self.possible_agents\n",
        "    }\n",
        "    # individual actions collection\n",
        "    self.individual_preds = {\n",
        "        agent: [] for agent in self.possible_agents\n",
        "    }\n",
        "\n",
        "    # render mode\n",
        "    self.render_mode = render_mode\n",
        "\n",
        "  def collect_individual_preds(self):\n",
        "    self.individual_preds = {\n",
        "        agent: [] for agent in self.possible_agents\n",
        "    }\n",
        "    for i in range(self.end_day+1):\n",
        "      # states are somewhat static\n",
        "      curr_data = self.df.loc[i,:]\n",
        "      curr_covs = [[x[0][0] for x in curr_data['cov_list']]]\n",
        "      curr_state = np.append(np.array(curr_covs), [curr_data[tech].values.tolist() for tech in self.tech_indicator_list ], axis=0)\n",
        "      for agent in self.possible_agents:\n",
        "        action, _states = self.agent_name_mapping[agent].predict(curr_state)\n",
        "        self.individual_preds[agent].append(action)\n",
        "  \n",
        "  def step(self, actions):\n",
        "    # print(self.day)\n",
        "    # print(actions)\n",
        "\n",
        "    # termination\n",
        "    # check: termination determined correctly?\n",
        "    self.terminal = self.day[self.training_agent] >= self.end_day\n",
        "    # self.terminal = {\n",
        "    #     agent: self.day >= len(self.df.index.unique())/self.stock_dim-1 for agent in self.possible_agents\n",
        "    # }\n",
        "\n",
        "    if self.terminal:\n",
        "      print(\"=================================\")\n",
        "      print(\"begin_total_asset:{}\".format(self.asset_memory[self.training_agent][0]))           \n",
        "      # print(\"end_total_asset:{}\".format(self.portfolio_value[self.training_agent]))\n",
        "      print(\"end_total_asset:{}\".format(self.total_value_memory[self.training_agent][0]))\n",
        "\n",
        "      df_daily_return = pd.DataFrame(self.portfolio_return_memory[self.training_agent])\n",
        "      # df_daily_pv = pd.DataFrame(self.total_value_memory[self.training_agent])\n",
        "      df_daily_return.columns = ['daily_return']\n",
        "      # df_daily_pv.columns = ['daily_pv']\n",
        "      if df_daily_return['daily_return'].std() !=0:\n",
        "        sharpe = (252**0.5)*df_daily_return['daily_return'].mean() / df_daily_return['daily_return'].std()\n",
        "      # if df_daily_pv['daily_pv'].std() !=0:\n",
        "      #   sharpe = (252**0.5)*df_daily_pv['daily_pv'].mean() / df_daily_pv['daily_pv'].std()\n",
        "        print(\"Sharpe: \",sharpe)\n",
        "      print(\"=================================\")\n",
        "\n",
        "      return self.state[self.training_agent], self.reward[self.training_agent], self.terminal, {}\n",
        "\n",
        "    else:\n",
        "      # loop through all agents so that each of them predict an action (portfolio weights)\n",
        "      for agent in self.possible_agents:\n",
        "        # get action\n",
        "        if agent == self.training_agent:\n",
        "          action = actions\n",
        "        else:\n",
        "          # action, _states = self.agent_name_mapping[agent].predict(self.state[agent], deterministic=False)\n",
        "          action = self.individual_preds[agent][self.day[agent]]\n",
        "\n",
        "        # normalisation\n",
        "        weights = self.softmax_normalization(action) \n",
        "\n",
        "        # stock ratio - buy/sell/hold\n",
        "        prev_stock_ratio = self.actions_memory[agent][-1]\n",
        "        diff_stock_ratio = prev_stock_ratio - weights\n",
        "\n",
        "        # money - increase if sell, decrease if buy, no changes if hold\n",
        "        prev_money = self.money_memory[agent][-1]\n",
        "        curr_money = prev_money + sum(diff_stock_ratio * self.stock_volume_reference * self.data[agent][\"Close\"].values)\n",
        "        self.money_memory[agent].append(curr_money)\n",
        "\n",
        "        # total value - money + currently held stock value\n",
        "        curr_total = curr_money + sum(weights * self.stock_volume_reference * self.data[agent][\"Close\"].values)\n",
        "        self.total_value_memory[agent].append(curr_total)\n",
        "\n",
        "        # actions memory\n",
        "        self.actions_memory[agent].append(weights)\n",
        "        last_day_memory = self.data[agent]\n",
        "\n",
        "        # load next state\n",
        "        self.day[agent] += 1\n",
        "        self.data[agent] = self.df.loc[self.day[agent],:]\n",
        "        self.covs[agent] = [[x[0][0] for x in self.data[agent]['cov_list']]]\n",
        "        self.state[agent] =  np.append(np.array(self.covs[agent]), [self.data[agent][tech].values.tolist() for tech in self.tech_indicator_list ], axis=0)\n",
        "        \n",
        "        # calculate portfolio return\n",
        "        # individual stocks' return * weight\n",
        "        portfolio_return = sum(((self.data[agent][\"Close\"].values / last_day_memory[\"Close\"].values)-1)*weights)\n",
        "        \n",
        "        # update portfolio value\n",
        "        # todo: to fix to our version?\n",
        "        # new_portfolio_value = self.portfolio_value[agent]*(1+portfolio_return)\n",
        "        # self.portfolio_value[agent] = new_portfolio_value\n",
        "\n",
        "        # save into memory\n",
        "        self.portfolio_return_memory[agent].append(portfolio_return)\n",
        "        self.cum_portfolio_return_memory[agent].append(self.cum_portfolio_return_memory[agent][-1] + portfolio_return)\n",
        "        self.date_memory[agent].append(self.data[agent][\"Date\"].unique()[0])            \n",
        "        self.asset_memory[agent].append(curr_total)\n",
        "\n",
        "        # the reward is the new portfolio value or end portfolio value\n",
        "        self.reward[agent] = curr_total \n",
        "        #self.reward = self.reward*self.reward_scaling\n",
        "      \n",
        "      # penalise or reward the target agent based on the result of all other agents\n",
        "      all_rewards = list(self.reward.values())\n",
        "      for agent in self.possible_agents:\n",
        "        # ratio = current agent / other agent\n",
        "        # if reward of current agent > other agent, ratio > 1, reward is increased\n",
        "        # else ratio < 1, reward is penalised\n",
        "        self.reward[agent] *= prod(self.reward[agent] / all_rewards)\n",
        "\n",
        "        # if money on hand is negative, large penalty is applied as this is unwanted\n",
        "        if self.money_memory[agent][-1] < 0:\n",
        "          self.reward[agent] *= -1\n",
        "        # self.reward[self.training_agent] *= (self.reward[self.training_agent] / pv)\n",
        "\n",
        "    return self.state[agent], self.reward[self.training_agent], self.terminal, {}\n",
        "\n",
        "  def reset(self, seed=None, return_info=False, options=None):\n",
        "    # print(\"reset\")\n",
        "\n",
        "    # agents\n",
        "    self.agents = self.possible_agents[:]\n",
        "\n",
        "    # attributes\n",
        "    self.day = {\n",
        "        agent: 0 for agent in self.possible_agents\n",
        "    }\n",
        "    self.data = {\n",
        "        agent: self.df.loc[self.day[agent],:] for agent in self.possible_agents\n",
        "    }\n",
        "    self.covs = {\n",
        "        agent: [[x[0][0] for x in self.data[agent]['cov_list']]] for agent in self.possible_agents\n",
        "    }\n",
        "    self.state = {\n",
        "        agent: np.append(np.array(self.covs[agent]), [self.data[agent][tech].values.tolist() for tech in self.tech_indicator_list ], axis=0) for agent in self.possible_agents\n",
        "    }\n",
        "\n",
        "    # memory\n",
        "    self.portfolio_value = {\n",
        "        agent: self.initial_amount for agent in self.possible_agents\n",
        "    }\n",
        "    self.asset_memory = {\n",
        "        agent: [self.initial_amount] for agent in self.possible_agents\n",
        "    }\n",
        "    self.portfolio_return_memory = {\n",
        "        agent: [0] for agent in self.possible_agents\n",
        "    }\n",
        "    self.cum_portfolio_return_memory = {\n",
        "        agent: [0] for agent in self.possible_agents\n",
        "    }\n",
        "\n",
        "    # stock ratio\n",
        "    self.actions_memory = {\n",
        "        agent: [[0]*self.stock_dim] for agent in self.possible_agents\n",
        "    }\n",
        "    self.date_memory = {\n",
        "        agent: [self.data[agent][\"Date\"].unique()[0]] for agent in self.possible_agents\n",
        "    }\n",
        "\n",
        "    # free cash\n",
        "    self.money_memory = {\n",
        "        agent: [self.initial_amount] for agent in self.possible_agents\n",
        "    }\n",
        "    # cash + stock value\n",
        "    self.total_value_memory = {\n",
        "        agent: [self.initial_amount] for agent in self.possible_agents\n",
        "    }\n",
        "\n",
        "    # rewards\n",
        "    self.reward = {\n",
        "        agent: None for agent in self.possible_agents\n",
        "    }\n",
        "    \n",
        "    # misc\n",
        "    self.terminal = False \n",
        "    #self.cost = 0\n",
        "    #self.trades = 0\n",
        "    \n",
        "    return self.state[self.training_agent] \n",
        "\n",
        "  def render(self):\n",
        "    return self.state[self.training_agent]\n",
        "\n",
        "  def seed(self, seed=None):\n",
        "    self.np_random, seed = seeding.np_random(seed)\n",
        "    return [seed]\n",
        "  \n",
        "  def softmax_normalization(self, actions):\n",
        "    numerator = np.exp(actions)\n",
        "    denominator = np.sum(np.exp(actions))\n",
        "    softmax_output = numerator/denominator\n",
        "    return softmax_output\n",
        "\n",
        "  def set_training_agent(self, agent):\n",
        "    # print(agent)\n",
        "    self.training_agent = agent\n",
        "\n",
        "  def learn(self, total_timesteps=1000):\n",
        "    init_pv = {}\n",
        "    final_pv = {}\n",
        "    init_cash = {}\n",
        "    final_cash = {}\n",
        "    init_daily_pr = {}\n",
        "    final_daily_pr = {}\n",
        "    init_cum_pr = {}\n",
        "    final_cum_pr = {}\n",
        "    sharpe_ratio = {\n",
        "        agent: [] for agent in self.possible_agents\n",
        "    }\n",
        "\n",
        "    # run till terminal in each timestep\n",
        "    for n in range(total_timesteps):\n",
        "      print(\"Step:\", n+1)\n",
        "      self.collect_individual_preds()\n",
        "      for agent in self.possible_agents:\n",
        "        self.set_training_agent(agent)\n",
        "        self.agent_name_mapping[agent] = self.agent_name_mapping[agent].learn(total_timesteps=1)\n",
        "        sharpe_ratio[agent].append(calculate_sharpe(self.portfolio_return_memory[agent]))\n",
        "        \n",
        "      if n == 0:\n",
        "        # save init for plot\n",
        "        for agent in self.possible_agents:\n",
        "          init_pv[agent] = self.total_value_memory[agent]\n",
        "          init_daily_pr[agent] = self.portfolio_return_memory[agent]\n",
        "          init_cum_pr[agent] = self.cum_portfolio_return_memory[agent]\n",
        "          init_cash[agent] = self.money_memory[agent]\n",
        "    \n",
        "    # save final for plot\n",
        "    for agent in self.possible_agents:\n",
        "      final_pv[agent] = self.total_value_memory[agent]\n",
        "      final_daily_pr[agent] = self.portfolio_return_memory[agent]\n",
        "      final_cum_pr[agent] = self.cum_portfolio_return_memory[agent]\n",
        "      final_cash[agent] = self.money_memory[agent]\n",
        "    \n",
        "    return init_pv, final_pv, init_cash, final_cash, init_daily_pr, final_daily_pr, init_cum_pr, final_cum_pr, sharpe_ratio\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### A2C"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "a2c_comp_env = CompetitivePMEnv(df=mixed_df, algo_type=\"A2C\", **env_kwargs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "(\n",
        "    a2c_comp_init_pv, \n",
        "    a2c_comp_final_pv, \n",
        "    a2c_comp_init_cash, \n",
        "    a2c_comp_final_cash, \n",
        "    a2c_comp_init_daily_pr, \n",
        "    a2c_comp_final_daily_pr, \n",
        "    a2c_comp_init_cum_pr, \n",
        "    a2c_comp_final_cum_pr, \n",
        "    a2c_comp_sr\n",
        ") = a2c_comp_env.learn(\n",
        "    total_timesteps=training_timesteps,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "save_to_json(\"a2c_comp_init_pv_{}.json\".format(training_timesteps), a2c_comp_init_pv)\n",
        "save_to_json(\"a2c_comp_final_pv_{}.json\".format(training_timesteps), a2c_comp_final_pv)\n",
        "\n",
        "save_to_json(\"a2c_comp_init_cash_{}.json\".format(training_timesteps), a2c_comp_init_cash)\n",
        "save_to_json(\"a2c_comp_final_cash_{}.json\".format(training_timesteps), a2c_comp_final_cash)\n",
        "\n",
        "save_to_json(\"a2c_comp_init_daily_pr_{}.json\".format(training_timesteps), a2c_comp_init_daily_pr)\n",
        "save_to_json(\"a2c_comp_final_daily_pr_{}.json\".format(training_timesteps), a2c_comp_final_daily_pr)\n",
        "\n",
        "save_to_json(\"a2c_comp_init_cum_pr_{}.json\".format(training_timesteps), a2c_comp_init_cum_pr)\n",
        "save_to_json(\"a2c_comp_final_cum_pr_{}.json\".format(training_timesteps), a2c_comp_final_cum_pr)\n",
        "\n",
        "save_to_json(\"a2c_comp_sr_{}.json\".format(training_timesteps), a2c_comp_sr)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### PPO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ppo_comp_env = CompetitivePMEnv(df=mixed_df, algo_type=\"PPO\", **env_kwargs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "(\n",
        "    ppo_comp_init_pv, \n",
        "    ppo_comp_final_pv, \n",
        "    ppo_comp_init_cash, \n",
        "    ppo_comp_final_cash, \n",
        "    ppo_comp_init_daily_pr, \n",
        "    ppo_comp_final_daily_pr, \n",
        "    ppo_comp_init_cum_pr, \n",
        "    ppo_comp_final_cum_pr, \n",
        "    ppo_comp_sr\n",
        ") = ppo_comp_env.learn(\n",
        "    total_timesteps=training_timesteps,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "save_to_json(\"ppo_comp_init_pv_{}.json\".format(training_timesteps), ppo_comp_init_pv)\n",
        "save_to_json(\"ppo_comp_final_pv_{}.json\".format(training_timesteps), ppo_comp_final_pv)\n",
        "\n",
        "save_to_json(\"ppo_comp_init_cash_{}.json\".format(training_timesteps), ppo_comp_init_cash)\n",
        "save_to_json(\"ppo_comp_final_cash_{}.json\".format(training_timesteps), ppo_comp_final_cash)\n",
        "\n",
        "save_to_json(\"ppo_comp_init_daily_pr_{}.json\".format(training_timesteps), ppo_comp_init_daily_pr)\n",
        "save_to_json(\"ppo_comp_final_daily_pr_{}.json\".format(training_timesteps), ppo_comp_final_daily_pr)\n",
        "\n",
        "save_to_json(\"ppo_comp_init_cum_pr_{}.json\".format(training_timesteps), ppo_comp_init_cum_pr)\n",
        "save_to_json(\"ppo_comp_final_cum_pr_{}.json\".format(training_timesteps), ppo_comp_final_cum_pr)\n",
        "\n",
        "save_to_json(\"ppo_comp_sr_{}.json\".format(training_timesteps), ppo_comp_sr)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cooperative"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CooperativePMEnv(gym.Env):\n",
        "  metadata = {\"render_modes\": [\"human\"], \"name\": \"marlpm_v1\"}\n",
        "\n",
        "  def __init__(\n",
        "      self, \n",
        "      df,\n",
        "      stock_dim,\n",
        "      hmax,\n",
        "      initial_amount,\n",
        "      transaction_cost_pct,\n",
        "      reward_scaling,\n",
        "      state_space,\n",
        "      action_space,\n",
        "      tech_indicator_list,\n",
        "      turbulence_threshold=None,\n",
        "      lookback=252,\n",
        "      day=0,\n",
        "      render_mode=None,\n",
        "      algo_type=\"Mixed\", # default=mixed\n",
        "  ):\n",
        "\n",
        "    assert algo_type in [\"A2C\", \"DDPG\", \"PPO\", \"Mixed\"]\n",
        "\n",
        "    # attributes\n",
        "    self.lookback=lookback\n",
        "    self.df = df\n",
        "    self.stock_dim = stock_dim\n",
        "    self.hmax = hmax\n",
        "    self.initial_amount = initial_amount\n",
        "    self.transaction_cost_pct =transaction_cost_pct\n",
        "    self.reward_scaling = reward_scaling\n",
        "    self.state_space = state_space\n",
        "    self.action_dim = action_space\n",
        "    self.tech_indicator_list = tech_indicator_list\n",
        "    self.possible_agents = TYPE_TO_NAMES[algo_type]\n",
        "    \n",
        "    # buy/sell ratio reference, to explore\n",
        "    self.end_day = len(self.df.index.unique()) - 1\n",
        "    self.stock_volume_reference = 10000\n",
        "\n",
        "    # spaces\n",
        "    # check: spaces for observations only? sharing will affect?\n",
        "    self.action_space = gym.spaces.Box(low = -1, high = 1, shape = (self.action_dim,))\n",
        "    self.observation_space = gym.spaces.Box(low=0, high=np.inf, shape = (1+len(self.tech_indicator_list), self.state_space))\n",
        "\n",
        "    # agents\n",
        "    self.agent_name_mapping = {\n",
        "        # agent: AGENT_STR_TO_OBJECT[agent](policy, self) for agent in self.possible_agents\n",
        "        agent: AGENT_STR_TO_OBJECT[algo_type](policy, self, n_steps=self.end_day) for agent in self.possible_agents\n",
        "    }\n",
        "    self.training_agent = None\n",
        "    self.day = {\n",
        "        agent: day for agent in self.possible_agents\n",
        "    }\n",
        "\n",
        "    # data\n",
        "    self.data = {\n",
        "        agent: self.df.loc[self.day[agent],:] for agent in self.possible_agents\n",
        "    }\n",
        "    self.covs = {\n",
        "        agent: [[x[0][0] for x in self.data[agent]['cov_list']]] for agent in self.possible_agents\n",
        "    }\n",
        "    self.state = {\n",
        "        agent: np.append(np.array(self.covs[agent]), [self.data[agent][tech].values.tolist() for tech in self.tech_indicator_list ], axis=0) for agent in self.possible_agents\n",
        "    }\n",
        "    self.terminal = False     \n",
        "    self.turbulence_threshold = turbulence_threshold   \n",
        "\n",
        "    # memory\n",
        "    self.portfolio_value = {\n",
        "        agent: self.initial_amount for agent in self.possible_agents\n",
        "    }\n",
        "    self.asset_memory = {\n",
        "        agent: [self.initial_amount] for agent in self.possible_agents\n",
        "    }\n",
        "    self.portfolio_return_memory = {\n",
        "        agent: [0] for agent in self.possible_agents\n",
        "    }\n",
        "    self.cum_portfolio_return_memory = {\n",
        "        agent: [0] for agent in self.possible_agents\n",
        "    }\n",
        "\n",
        "    # stock ratio\n",
        "    self.actions_memory = {\n",
        "        agent: [[0]*self.stock_dim] for agent in self.possible_agents\n",
        "    }\n",
        "    self.date_memory = {\n",
        "        agent: [self.data[agent][\"Date\"].unique()[0]] for agent in self.possible_agents\n",
        "    }\n",
        "\n",
        "    # free cash\n",
        "    self.money_memory = {\n",
        "        agent: [self.initial_amount] for agent in self.possible_agents\n",
        "    }\n",
        "    # cash + stock value\n",
        "    self.total_value_memory = {\n",
        "        agent: [self.initial_amount] for agent in self.possible_agents\n",
        "    }\n",
        "    # individual actions collection\n",
        "    self.individual_preds = {\n",
        "        agent: [] for agent in self.possible_agents\n",
        "    }\n",
        "\n",
        "    # render mode\n",
        "    self.render_mode = render_mode\n",
        "\n",
        "  def collect_individual_preds(self):\n",
        "    self.individual_preds = {\n",
        "        agent: [] for agent in self.possible_agents\n",
        "    }\n",
        "    for i in range(self.end_day+1):\n",
        "      # states are somewhat static\n",
        "      curr_data = self.df.loc[i,:]\n",
        "      curr_covs = [[x[0][0] for x in curr_data['cov_list']]]\n",
        "      curr_state = np.append(np.array(curr_covs), [curr_data[tech].values.tolist() for tech in self.tech_indicator_list ], axis=0)\n",
        "      for agent in self.possible_agents:\n",
        "        action, _states = self.agent_name_mapping[agent].predict(curr_state)\n",
        "        self.individual_preds[agent].append(action)\n",
        "  \n",
        "  def step(self, actions):\n",
        "    # print(self.day)\n",
        "    # print(actions)\n",
        "\n",
        "    # termination\n",
        "    # check: termination determined correctly?\n",
        "    self.terminal = self.day[self.training_agent] >= self.end_day\n",
        "    # self.terminal = {\n",
        "    #     agent: self.day >= len(self.df.index.unique())/self.stock_dim-1 for agent in self.possible_agents\n",
        "    # }\n",
        "\n",
        "    if self.terminal:\n",
        "      print(\"=================================\")\n",
        "      print(\"begin_total_asset:{}\".format(self.asset_memory[self.training_agent][0]))           \n",
        "      # print(\"end_total_asset:{}\".format(self.portfolio_value[self.training_agent]))\n",
        "      print(\"end_total_asset:{}\".format(self.total_value_memory[self.training_agent][0]))\n",
        "\n",
        "      df_daily_return = pd.DataFrame(self.portfolio_return_memory[self.training_agent])\n",
        "      # df_daily_pv = pd.DataFrame(self.total_value_memory[self.training_agent])\n",
        "      df_daily_return.columns = ['daily_return']\n",
        "      # df_daily_pv.columns = ['daily_pv']\n",
        "      if df_daily_return['daily_return'].std() !=0:\n",
        "        sharpe = (252**0.5)*df_daily_return['daily_return'].mean() / df_daily_return['daily_return'].std()\n",
        "      # if df_daily_pv['daily_pv'].std() !=0:\n",
        "      #   sharpe = (252**0.5)*df_daily_pv['daily_pv'].mean() / df_daily_pv['daily_pv'].std()\n",
        "        print(\"Sharpe: \",sharpe)\n",
        "      print(\"=================================\")\n",
        "\n",
        "      return self.state[self.training_agent], self.reward[self.training_agent], self.terminal, {}\n",
        "\n",
        "    else:\n",
        "      # loop through all agents so that each of them predict an action (portfolio weights)\n",
        "      for agent in self.possible_agents:\n",
        "        # get action\n",
        "        if agent == self.training_agent:\n",
        "          action = actions\n",
        "        else:\n",
        "          # action, _states = self.agent_name_mapping[agent].predict(self.state[agent], deterministic=False)\n",
        "          action = self.individual_preds[agent][self.day[agent]]\n",
        "\n",
        "        # normalisation\n",
        "        weights = self.softmax_normalization(action) \n",
        "\n",
        "        # stock ratio - buy/sell/hold\n",
        "        prev_stock_ratio = self.actions_memory[agent][-1]\n",
        "        diff_stock_ratio = prev_stock_ratio - weights\n",
        "\n",
        "        # money - increase if sell, decrease if buy, no changes if hold\n",
        "        prev_money = self.money_memory[agent][-1]\n",
        "        curr_money = prev_money + sum(diff_stock_ratio * self.stock_volume_reference * self.data[agent][\"Close\"].values)\n",
        "        self.money_memory[agent].append(curr_money)\n",
        "\n",
        "        # total value - money + currently held stock value\n",
        "        curr_total = curr_money + sum(weights * self.stock_volume_reference * self.data[agent][\"Close\"].values)\n",
        "        self.total_value_memory[agent].append(curr_total)\n",
        "\n",
        "        # actions memory\n",
        "        self.actions_memory[agent].append(weights)\n",
        "        last_day_memory = self.data[agent]\n",
        "\n",
        "        # load next state\n",
        "        self.day[agent] += 1\n",
        "        self.data[agent] = self.df.loc[self.day[agent],:]\n",
        "        self.covs[agent] = [[x[0][0] for x in self.data[agent]['cov_list']]]\n",
        "        self.state[agent] =  np.append(np.array(self.covs[agent]), [self.data[agent][tech].values.tolist() for tech in self.tech_indicator_list ], axis=0)\n",
        "        \n",
        "        # calculate portfolio return\n",
        "        # individual stocks' return * weight\n",
        "        portfolio_return = sum(((self.data[agent][\"Close\"].values / last_day_memory[\"Close\"].values)-1)*weights)\n",
        "        \n",
        "        # update portfolio value\n",
        "        # todo: to fix to our version?\n",
        "        # new_portfolio_value = self.portfolio_value[agent]*(1+portfolio_return)\n",
        "        # self.portfolio_value[agent] = new_portfolio_value\n",
        "\n",
        "        # save into memory\n",
        "        self.portfolio_return_memory[agent].append(portfolio_return)\n",
        "        self.cum_portfolio_return_memory[agent].append(self.cum_portfolio_return_memory[agent][-1] + portfolio_return)\n",
        "        self.date_memory[agent].append(self.data[agent][\"Date\"].unique()[0])            \n",
        "        self.asset_memory[agent].append(curr_total)\n",
        "\n",
        "        # the reward is the new portfolio value or end portfolio value\n",
        "        self.reward[agent] = curr_total \n",
        "        #self.reward = self.reward*self.reward_scaling\n",
        "      \n",
        "      # penalise or reward the target agent based on the result of all other agents\n",
        "      all_rewards = list(self.reward.values())\n",
        "      for agent in self.possible_agents:\n",
        "        # ratio = smaller reward / greater reward\n",
        "        # rewards have to be similar across all agents\n",
        "        # the greater the difference, the greater the penalty\n",
        "        self.reward[agent] *= prod([\n",
        "            pv / self.reward[agent]\n",
        "            if self.reward[agent] > pv\n",
        "            else self.reward[agent] / pv\n",
        "            for pv in all_rewards\n",
        "        ])\n",
        "\n",
        "        # if money on hand is negative, large penalty is applied as this is unwanted\n",
        "        if self.money_memory[agent][-1] < 0:\n",
        "          self.reward[agent] *= -1\n",
        "\n",
        "    return self.state[agent], self.reward[self.training_agent], self.terminal, {}\n",
        "\n",
        "  def reset(self, seed=None, return_info=False, options=None):\n",
        "    # print(\"reset\")\n",
        "\n",
        "    # agents\n",
        "    self.agents = self.possible_agents[:]\n",
        "\n",
        "    # attributes\n",
        "    self.day = {\n",
        "        agent: 0 for agent in self.possible_agents\n",
        "    }\n",
        "    self.data = {\n",
        "        agent: self.df.loc[self.day[agent],:] for agent in self.possible_agents\n",
        "    }\n",
        "    self.covs = {\n",
        "        agent: [[x[0][0] for x in self.data[agent]['cov_list']]] for agent in self.possible_agents\n",
        "    }\n",
        "    self.state = {\n",
        "        agent: np.append(np.array(self.covs[agent]), [self.data[agent][tech].values.tolist() for tech in self.tech_indicator_list ], axis=0) for agent in self.possible_agents\n",
        "    }\n",
        "\n",
        "    # memory\n",
        "    self.portfolio_value = {\n",
        "        agent: self.initial_amount for agent in self.possible_agents\n",
        "    }\n",
        "    self.asset_memory = {\n",
        "        agent: [self.initial_amount] for agent in self.possible_agents\n",
        "    }\n",
        "    self.portfolio_return_memory = {\n",
        "        agent: [0] for agent in self.possible_agents\n",
        "    }\n",
        "    self.cum_portfolio_return_memory = {\n",
        "        agent: [0] for agent in self.possible_agents\n",
        "    }\n",
        "\n",
        "    # stock ratio\n",
        "    self.actions_memory = {\n",
        "        agent: [[0]*self.stock_dim] for agent in self.possible_agents\n",
        "    }\n",
        "    self.date_memory = {\n",
        "        agent: [self.data[agent][\"Date\"].unique()[0]] for agent in self.possible_agents\n",
        "    }\n",
        "\n",
        "    # free cash\n",
        "    self.money_memory = {\n",
        "        agent: [self.initial_amount] for agent in self.possible_agents\n",
        "    }\n",
        "    # cash + stock value\n",
        "    self.total_value_memory = {\n",
        "        agent: [self.initial_amount] for agent in self.possible_agents\n",
        "    }\n",
        "\n",
        "    # rewards\n",
        "    self.reward = {\n",
        "        agent: None for agent in self.possible_agents\n",
        "    }\n",
        "    \n",
        "    # misc\n",
        "    self.terminal = False \n",
        "    #self.cost = 0\n",
        "    #self.trades = 0\n",
        "    \n",
        "    return self.state[self.training_agent] \n",
        "\n",
        "  def render(self):\n",
        "    return self.state[self.training_agent]\n",
        "\n",
        "  def seed(self, seed=None):\n",
        "    self.np_random, seed = seeding.np_random(seed)\n",
        "    return [seed]\n",
        "  \n",
        "  def softmax_normalization(self, actions):\n",
        "    numerator = np.exp(actions)\n",
        "    denominator = np.sum(np.exp(actions))\n",
        "    softmax_output = numerator/denominator\n",
        "    return softmax_output\n",
        "\n",
        "  def set_training_agent(self, agent):\n",
        "    # print(agent)\n",
        "    self.training_agent = agent\n",
        "\n",
        "  def learn(self, total_timesteps=1000):\n",
        "    init_pv = {}\n",
        "    final_pv = {}\n",
        "    init_cash = {}\n",
        "    final_cash = {}\n",
        "    init_daily_pr = {}\n",
        "    final_daily_pr = {}\n",
        "    init_cum_pr = {}\n",
        "    final_cum_pr = {}\n",
        "    sharpe_ratio = {\n",
        "        agent: [] for agent in self.possible_agents\n",
        "    }\n",
        "\n",
        "    # run till terminal in each timestep\n",
        "    for n in range(total_timesteps):\n",
        "      print(\"Step:\", n+1)\n",
        "      self.collect_individual_preds()\n",
        "      for agent in self.possible_agents:\n",
        "        self.set_training_agent(agent)\n",
        "        self.agent_name_mapping[agent] = self.agent_name_mapping[agent].learn(total_timesteps=1)\n",
        "        sharpe_ratio[agent].append(calculate_sharpe(self.portfolio_return_memory[agent]))\n",
        "        \n",
        "      if n == 0:\n",
        "        # save init for plot\n",
        "        for agent in self.possible_agents:\n",
        "          init_pv[agent] = self.total_value_memory[agent]\n",
        "          init_daily_pr[agent] = self.portfolio_return_memory[agent]\n",
        "          init_cum_pr[agent] = self.cum_portfolio_return_memory[agent]\n",
        "          init_cash[agent] = self.money_memory[agent]\n",
        "    \n",
        "    # save final for plot\n",
        "    for agent in self.possible_agents:\n",
        "      final_pv[agent] = self.total_value_memory[agent]\n",
        "      final_daily_pr[agent] = self.portfolio_return_memory[agent]\n",
        "      final_cum_pr[agent] = self.cum_portfolio_return_memory[agent]\n",
        "      final_cash[agent] = self.money_memory[agent]\n",
        "    \n",
        "    return init_pv, final_pv, init_cash, final_cash, init_daily_pr, final_daily_pr, init_cum_pr, final_cum_pr, sharpe_ratio\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### A2C"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "a2c_coop_env = CooperativePMEnv(df=mixed_df, algo_type=\"A2C\", **env_kwargs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "(\n",
        "    a2c_coop_init_pv, \n",
        "    a2c_coop_final_pv, \n",
        "    a2c_coop_init_cash, \n",
        "    a2c_coop_final_cash, \n",
        "    a2c_coop_init_daily_pr, \n",
        "    a2c_coop_final_daily_pr, \n",
        "    a2c_coop_init_cum_pr, \n",
        "    a2c_coop_final_cum_pr, \n",
        "    a2c_coop_sr\n",
        ") = a2c_coop_env.learn(\n",
        "    total_timesteps=training_timesteps,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "save_to_json(\"a2c_coop_init_pv_{}.json\".format(training_timesteps), a2c_coop_init_pv)\n",
        "save_to_json(\"a2c_coop_final_pv_{}.json\".format(training_timesteps), a2c_coop_final_pv)\n",
        "\n",
        "save_to_json(\"a2c_coop_init_cash_{}.json\".format(training_timesteps), a2c_coop_init_cash)\n",
        "save_to_json(\"a2c_coop_final_cash_{}.json\".format(training_timesteps), a2c_coop_final_cash)\n",
        "\n",
        "save_to_json(\"a2c_coop_init_daily_pr_{}.json\".format(training_timesteps), a2c_coop_init_daily_pr)\n",
        "save_to_json(\"a2c_coop_final_daily_pr_{}.json\".format(training_timesteps), a2c_coop_final_daily_pr)\n",
        "\n",
        "save_to_json(\"a2c_coop_init_cum_pr_{}.json\".format(training_timesteps), a2c_coop_init_cum_pr)\n",
        "save_to_json(\"a2c_coop_final_cum_pr_{}.json\".format(training_timesteps), a2c_coop_final_cum_pr)\n",
        "\n",
        "save_to_json(\"a2c_coop_sr_{}.json\".format(training_timesteps), a2c_coop_sr)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### PPO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ppo_coop_env = CooperativePMEnv(df=mixed_df, algo_type=\"PPO\", **env_kwargs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "(\n",
        "    ppo_coop_init_pv, \n",
        "    ppo_coop_final_pv, \n",
        "    ppo_coop_init_cash, \n",
        "    ppo_coop_final_cash, \n",
        "    ppo_coop_init_daily_pr, \n",
        "    ppo_coop_final_daily_pr, \n",
        "    ppo_coop_init_cum_pr, \n",
        "    ppo_coop_final_cum_pr, \n",
        "    ppo_coop_sr\n",
        ") = ppo_coop_env.learn(\n",
        "    total_timesteps=training_timesteps,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "save_to_json(\"ppo_coop_init_pv_{}.json\".format(training_timesteps), ppo_coop_init_pv)\n",
        "save_to_json(\"ppo_coop_final_pv_{}.json\".format(training_timesteps), ppo_coop_final_pv)\n",
        "\n",
        "save_to_json(\"ppo_coop_init_cash_{}.json\".format(training_timesteps), ppo_coop_init_cash)\n",
        "save_to_json(\"ppo_coop_final_cash_{}.json\".format(training_timesteps), ppo_coop_final_cash)\n",
        "\n",
        "save_to_json(\"ppo_coop_init_daily_pr_{}.json\".format(training_timesteps), ppo_coop_init_daily_pr)\n",
        "save_to_json(\"ppo_coop_final_daily_pr_{}.json\".format(training_timesteps), ppo_coop_final_daily_pr)\n",
        "\n",
        "save_to_json(\"ppo_coop_init_cum_pr_{}.json\".format(training_timesteps), ppo_coop_init_cum_pr)\n",
        "save_to_json(\"ppo_coop_final_cum_pr_{}.json\".format(training_timesteps), ppo_coop_final_cum_pr)\n",
        "\n",
        "save_to_json(\"ppo_coop_sr_{}.json\".format(training_timesteps), ppo_coop_sr)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Plots"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Market Trend"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.plot(stocks[\"AAPL\"][\"Close\"].index, stocks[\"AAPL\"][\"Close\"], label=\"AAPL\")\n",
        "plt.plot(stocks[\"TSLA\"][\"Close\"].index, stocks[\"TSLA\"][\"Close\"], label=\"TSLA\")\n",
        "plt.title(\"Daily Closing Value\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## A2C"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Competitive"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Portfolio Value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_line_graph(a2c_comp_env, a2c_comp_env.date_memory[a2c_comp_env.possible_agents[0]], a2c_comp_init_pv, \"Portfolio Value before Training\")\n",
        "plot_line_graph(a2c_comp_env, a2c_comp_env.date_memory[a2c_comp_env.possible_agents[0]], a2c_comp_final_pv, \"Portfolio Value after Training\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Cash Movement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_line_graph(a2c_comp_env, a2c_comp_env.date_memory[a2c_comp_env.possible_agents[0]], a2c_comp_init_cash, \"Cash Movement before Training\")\n",
        "plot_line_graph(a2c_comp_env, a2c_comp_env.date_memory[a2c_comp_env.possible_agents[0]], a2c_comp_final_cash, \"Cash Movement after Training\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Daily Portfolio Return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_line_graph(a2c_comp_env, a2c_comp_env.date_memory[a2c_comp_env.possible_agents[0]], a2c_comp_init_daily_pr, \"Daily Portfoliio Return before Training\")\n",
        "plot_line_graph(a2c_comp_env, a2c_comp_env.date_memory[a2c_comp_env.possible_agents[0]], a2c_comp_final_daily_pr, \"Daily Portfoliio Return after Training\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Cumulative Portfolio Return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_line_graph(a2c_comp_env, a2c_comp_env.date_memory[a2c_comp_env.possible_agents[0]], a2c_comp_init_cum_pr, \"Cumulative Portfoliio Return before Training\")\n",
        "plot_line_graph(a2c_comp_env, a2c_comp_env.date_memory[a2c_comp_env.possible_agents[0]], a2c_comp_final_cum_pr, \"Cumulative Portfoliio Return after Training\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sharpe Ratio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_line_graph(a2c_comp_env, range(1, training_timesteps+1), a2c_comp_sr, \"Competitive Sharpe Ratio Trend\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Cooperative"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Portfolio Value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_line_graph(a2c_coop_env, a2c_coop_env.date_memory[a2c_coop_env.possible_agents[0]], a2c_coop_init_pv, \"Portfolio Value before Training\")\n",
        "plot_line_graph(a2c_coop_env, a2c_coop_env.date_memory[a2c_coop_env.possible_agents[0]], a2c_coop_final_pv, \"Portfolio Value after Training\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Cash Movement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_line_graph(a2c_coop_env, a2c_coop_env.date_memory[a2c_coop_env.possible_agents[0]], a2c_coop_init_cash, \"Cash Movement before Training\")\n",
        "plot_line_graph(a2c_coop_env, a2c_coop_env.date_memory[a2c_coop_env.possible_agents[0]], a2c_coop_final_cash, \"Cash Movement after Training\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Daily Portfolio Return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_line_graph(a2c_coop_env, a2c_coop_env.date_memory[a2c_coop_env.possible_agents[0]], a2c_coop_init_daily_pr, \"Daily Portfoliio Return before Training\")\n",
        "plot_line_graph(a2c_coop_env, a2c_coop_env.date_memory[a2c_coop_env.possible_agents[0]], a2c_coop_final_daily_pr, \"Daily Portfoliio Return after Training\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Cumulative Portfolio Return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_line_graph(a2c_coop_env, a2c_coop_env.date_memory[a2c_coop_env.possible_agents[0]], a2c_coop_init_cum_pr, \"Cumulative Portfolio Return before Training\")\n",
        "plot_line_graph(a2c_coop_env, a2c_coop_env.date_memory[a2c_coop_env.possible_agents[0]], a2c_coop_final_cum_pr, \"Cumulative Portfolio Return after Training\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sharpe Ratio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_line_graph(a2c_coop_env, range(1, training_timesteps+1), a2c_coop_sr, \"Cooperative Sharpe Ratio Trend\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## PPO"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Competitive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Portfolio Value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_line_graph(ppo_comp_env, ppo_comp_env.date_memory[ppo_comp_env.possible_agents[0]], ppo_comp_init_pv, \"Portfolio Value before Training\")\n",
        "plot_line_graph(ppo_comp_env, ppo_comp_env.date_memory[ppo_comp_env.possible_agents[0]], ppo_comp_final_pv, \"Portfolio Value after Training\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Cash Movement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_line_graph(ppo_comp_env, ppo_comp_env.date_memory[ppo_comp_env.possible_agents[0]], ppo_comp_init_cash, \"Cash Movement before Training\")\n",
        "plot_line_graph(ppo_comp_env, ppo_comp_env.date_memory[ppo_comp_env.possible_agents[0]], ppo_comp_final_cash, \"Cash Movement after Training\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Daily Portfolio Return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_line_graph(ppo_comp_env, ppo_comp_env.date_memory[ppo_comp_env.possible_agents[0]], ppo_comp_init_daily_pr, \"Daily Portfolio Return before Training\")\n",
        "plot_line_graph(ppo_comp_env, ppo_comp_env.date_memory[ppo_comp_env.possible_agents[0]], ppo_comp_final_daily_pr, \"Daily Portfolio Return after Training\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Cumulative Portfolio Return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_line_graph(ppo_comp_env, ppo_comp_env.date_memory[ppo_comp_env.possible_agents[0]], ppo_comp_init_cum_pr, \"Cumulative Portfolio Return before Training\")\n",
        "plot_line_graph(ppo_comp_env, ppo_comp_env.date_memory[ppo_comp_env.possible_agents[0]], ppo_comp_final_cum_pr, \"Cumulative Portfolio Return after Training\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sharpe Ratio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_line_graph(ppo_comp_env, range(1, training_timesteps+1), ppo_comp_sr, \"Competitive Sharpe Ratio Trend\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Cooperative"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Portfolio Value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_line_graph(ppo_coop_env, ppo_coop_env.date_memory[ppo_coop_env.possible_agents[0]], ppo_coop_init_pv, \"Portfolio Value before Training\")\n",
        "plot_line_graph(ppo_coop_env, ppo_coop_env.date_memory[ppo_coop_env.possible_agents[0]], ppo_coop_final_pv, \"Portfolio Value after Training\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Cash Movement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_line_graph(ppo_coop_env, ppo_coop_env.date_memory[ppo_coop_env.possible_agents[0]], ppo_coop_init_cash, \"Cash Movement before Training\")\n",
        "plot_line_graph(ppo_coop_env, ppo_coop_env.date_memory[ppo_coop_env.possible_agents[0]], ppo_coop_final_cash, \"Cash Movement after Training\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Daily Portfolio Return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_line_graph(ppo_coop_env, ppo_coop_env.date_memory[ppo_coop_env.possible_agents[0]], ppo_coop_init_daily_pr, \"Daily Portfolio Return before Training\")\n",
        "plot_line_graph(ppo_coop_env, ppo_coop_env.date_memory[ppo_coop_env.possible_agents[0]], ppo_coop_final_daily_pr, \"Daily Portfolio Return after Training\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Cumulative Portfolio Return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_line_graph(ppo_coop_env, ppo_coop_env.date_memory[ppo_coop_env.possible_agents[0]], ppo_coop_init_cum_pr, \"Cumulative Portfolio Return before Training\")\n",
        "plot_line_graph(ppo_coop_env, ppo_coop_env.date_memory[ppo_coop_env.possible_agents[0]], ppo_coop_final_cum_pr, \"Cumulative Portfolio Return after Training\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sharpe Ratio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_line_graph(ppo_coop_env, range(1, training_timesteps+1), ppo_coop_sr, \"Cooperative Sharpe Ratio Trend\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
